[
  {
    "objectID": "index.html#fa-hand-peace-hello-there-this-is-a-website-i-dont-always-have-a-chance-to-update-but-it-has-a-few-posts-and-some-ways-to-get-in-touch-with-me-if-you-need-to-cheers-fa-champagne-glasses",
    "href": "index.html#fa-hand-peace-hello-there-this-is-a-website-i-dont-always-have-a-chance-to-update-but-it-has-a-few-posts-and-some-ways-to-get-in-touch-with-me-if-you-need-to-cheers-fa-champagne-glasses",
    "title": "",
    "section": " hello there! this is a website I don‚Äôt always have a chance to update but it has a few posts and some ways to get in touch with me if you need to; cheers ",
    "text": "hello there! this is a website I don‚Äôt always have a chance to update but it has a few posts and some ways to get in touch with me if you need to; cheers"
  },
  {
    "objectID": "vitae.html",
    "href": "vitae.html",
    "title": "Romuald ZAMI",
    "section": "",
    "text": "I believe that the 21st century requires a more multifaceted toolkit if we hope to solve complex challenges‚Äìprocess and task oriented approaches balanced within a framework that fosters strategic thinking. That interdisciplinary relationships can be strengthened through active community participation‚Äìworking toward something with the shared value of doing good.\n I bring a lot of energy and enthusiasm to the projects that I become involved in. I thrive in situations that require imagination and innovation. My interests are varied; among my passions I would list: academic inquiry, classical and contemporary thought, the music and cuisine of other cultures, the arts, traveling, languages, and social justice. I believe that our most vulnerable deserve respect and to be treated with dignity and fairness.\n Ultimately I hope that the work I do is helpful‚Äìthat it enables discovery or provides clarity. I hope to bring together practical and principled approaches and I hope to further grow as this philosophy guides me.\n\n\n\n\n\n\n I am dedicated and hard working‚Äìa good communicator and active community participant. I work well on team efforts and projects with the ultimate aim of using data for social good.\n I am:\n\n creative\n honest\n friendly\n enthusiastic\n keen to learn new things and take on new roles\n reliable\n a lateral thinker\n motivated\n\n I am able to:\n\n speak and read French\n read and understand some German\n listen to and follow instructions accurately\n work cooperatively\n take on new challenges\n complete tasks\n support others\n identify new innovations and technologies while working toward implementing them\n be proactive\n\n\n\n\n\n\n\n Web Technologies: HTML, CSS, Javascript; Quarto publishing; Rmarkdown Websites, Dashboards in R with shiny/flexdashboard that support crosstalk, and interactive data visualization with Plotly; Static Site Generators, i.e., Pelican, Jekyll, and others; Traditional CMSs like Wordpress and Drupal; previous coursework in Information Architecture and UX/UI design.\n Coding: R, Python, and Javascript.\n OSs: macOS, Linux, Windows, and containers. Within the Linux ecosystem, I have experience with Debian based distributions as well as Red Hat Enterprise Linux. I love the terminal and wish vim bindings were universal.\n Data: Relational Databases (data modeling and SQL); Extract-Transfer-Load (ETL) and Extract-Load-Transfer (ELT) strategies for data engineering‚Äìspecifically with Apache Airflow for ETL and Data Build Tool (dbt) for ELT; information retrieval; taxonomies and ontologies; and, knowledge organization theory.\n Data Science: both R and Python are a big part of my workflow from reading in data, transforming and ‚Äòtidying‚Äô data, exploring and visualizing data, modeling, reporting, presenting, and tying it all together in a dashboard. In connection to R, I am a proponent of the tidyverse framework for its unifying API. With respect to Python, I enjoy using polars, pyjanitor, pymc, and plotly to work in equivalent ways. I‚Äôm honestly happy working in both languages.\n Metadata schemas: knowledge of RDF, OWL, SPARQL, AACR2, RDA, MODS, METS, and PREMIS\n\n\n\n\n\n\n University of North Texas | Master of Science in Information Science (2013) Information Systems track\n University of California, Los Angeles (UCLA) | Bachelor of Arts in French and comparative literature (2008 / double major)\n Universit√© de la Sorbonne Nouvelle (Paris III) | Student exchange (2006‚Äì2007)\n\n\n\n\n\n\n\n\n\n\n using expert knowledge of and experience in data, statistical modeling, and machine learning methods; programming languages, packages/libraries, and big-data engineering solutions; manage and implement highly technical and complex data science projects from defining the vision to operationalizing solutions to ensure desired outcomes are achieved.\n\n\n provides both technical and administrative supervision to the DSS team of four to five staff (on average) as we plan, develop, and implement data science projects, data engineering pipelines, and dashboards/visual analytics. The DSS team is comprised of two Data Scientists, a Jr.¬†Data Scientist, a Data Engineer, and a Product Manager. We utilize the SCRUM framework to monitor and direct tasks, boards to communicate and evaluate progress, peer-to-peer code review in GitHub via pull request rules, and one on one sessions for pair programming and additional mentoring.\n\n\n work closely with both senior Public Health Information Systems (PHIS) IT staff and Internal Services Department (ISD) IT staff on a variety of data infrastructure projects including the management and maintenance of our data science tooling (both the applications and the servers); work strategically on data engineering bottlenecks to enhance reliability and scalability of data; use knowledge of data science and data engineering practices to add suggestions for burgeoning information management and governance work; act as technical resource for team leads interested in leveraging DSS tooling.\n\n\n building a robust R and Python community by collaborating closely with our RStudio partners to bring trainings to more than 180 staff within ACDC; helping to answer questions and support team members that run into issues related to version control, CI/CD, RStudio Connect, and other DSS tools; evangelize data modernization initiatives whenever possible while also staying up to date on the changing data science landscape.\n\n\n led DSS team to refactor (rewrite) from SAS and R codebases to large-scale (big data) architecture code that is automated with the Apache Airflow workflow tool leveraging Apache Spark via Python; these processes in some instances are three to four times faster than the original codebases, which we plan to continue to leverage for all of our Extract-Transfer-Load (ETL) needs.\n\n\n continuing to define and formalize the road map for data science within public health as an embedded group of expert resources that can consult and collaborate on a divisional or departmental opportunities/gaps in infrastructure or data engineering we can recommend plans for strengthening, streamlining, or augmenting.\n\n\n\n\n\n\n\n\n spearheading data integration projects with Whole Person Care program; optimizing links to internal master data management, countywide master data management, community/clinic based HIEs, and MCOs.\n\n\n\n help prototype operational dashboards for analyst team to put into production, working with clinical staff on creating tools that will add value rather than distract or oversaturate.\n\n\n work with leadership on identifying KPIs and other operationally valuable markers for high level dashboard applications that allow users to explore and interact adding business value.\n\n\n liaise with external county departments, HIEs, and MCOs regarding data sharing standards and governance for WPC population‚Äìup to speed on current federal and state regulations for what can and what cannot be shared depending on context.\n\n\n assist in authoring analysis billing rules for data team to use to correctly produce mandatory reporting given complexities of WPC funding via Medi-Cal waiver program and shifting horizon of competing and complimentary programming.\n\n\n responsible for calculating program milestones and incentives for state billing‚Äìworking with front line program staff to improve workflows and capture the data the demonstrates achievement for the WPC program.\n\n\n act as bridge between IT, data team, evaluation team, and leadership on all data related matters‚Äìworking to improve programming workflows through version control and software engineering best practices.\n\n\n\n\n\n\n\n\n provide strategic oversight to organization‚Äôs data collection systems and reporting processes through the collection and analysis of data, reporting on and capturing of outcome metrics, and providing input to create a more data-driven culture\n\n\n responsible for identifying, monitoring, and evaluating program and organizational outcomes to ensure compliance and effectiveness of programming\n\n\n moved social service database from a no access closed system to AWS to reduce organizational cost and optimize IT ecosystem\n\n\n liaised with internal departments for reporting/proposal questions, and to answer questions about resident population trends, funding and compliance layers for affordable housing; and, how the two work toward a broader outcomes strategy\n\n\n prepare recurring and special aggregated data reports and related decision support tools for organization to support analytics value chain: datareportinganalysisactionvalue\n\n\n oversaw the application and implementation of assessment procedures for longitudinal data collection; provided training in the use of survey instruments and answers to questions about what is tracked and why\n\n\n leveraged agglomerative clustering models, survival models, generalized linear models, and binary classifiers like logistic regression in order to build a wider understanding of how people fare in permanent supportive housing to see if we can detect negative outcomes early and target support services early.\n\n\n converted all data workflows to version control via GitHub for all projects including web application and some of the data engineering\n\n\n liaise with external organizations, research institutions, and contractors / consultants interested in or currently conducting research with organization to answer questions about what we currently are using to track outcomes / offer support\n\n\n\n\n\n\n\n\n provide in-depth analysis of both the resident population and underlying subsidy and compliance structure for entire portfolio\n\n\n leverage validated research tools to improve data collection; providing a pathway for future program evaluation based on real measurements that have reliable sensitivity and specificity\n\n\n explored open-source database alternatives to reduce organizational cost and fit within the IT ecosystem\n\n\n actively participated in internal organizational dialog around the need to pivot from demonstrating need to demonstrating efficacy‚Äìchampioned the role data could play in program development, intervention, and outcomes\n\n\n prepared recurring and special aggregated data reports for asset management compliance, annual organizational audit, resident programs grant reporting, and housing development RFP/RSFQ submissions\n\n\n\n assist in any data related projects with community partners and research institutions\n\n\n\n\n\n\n\n\n gathered baseline data on current resident enrollment in Medi-Cal, their health care utilization and access, history of chronic health conditions, as well as food intake.\n\n\n performed statistical analysis of health survey data using Python‚Äìwe had 200 respondents and were able to make programmatic changes based on our findings\n\n\n developed and implemented health training workshops and on-on-one guidance for Program Managers and Resident Services Coordinators to improve knowledge of new Affordable Care Act-driven insurance enrollment challenges for residents with advanced conditions\n\n\n implemented resident-focused trainings on managing and seeking help for chronic health conditions as well as creating health literacy guides for residents\n\n\n planed and coordinated resident health fairs in collaboration with community health providers to improve health literacy of residents in connection to both chronic conditions as well as the resources in their community\n\n\n provided coordinating support to Program Managers and staff charged with oversight of existing on-site health resources and wellness programs. Explored potential partnerships to provide residents improved health and well-being\n\n\n prepared and maintained accurate reports and survey data utilizing the Efforts to Outcomes database\n\n\n project troubleshooter par excellence.\n\n\n\n\n\n\n\n\n responsible for administering and uploading survey data to the Substance Abuse and Mental Health Services Administration (SAMHSA) and its affiliates while maintaining confidentiality and records keeping due diligence\n\n\n worked with Resident Service Coordinators, programmatic staff, and grant evaluator on collecting and maintaining data pertinent mandated progress reports to SAMHSA\n\n\n researched, wrote, and compiled bi-annual reports for the Cooperative Agreement to Benefit Homeless Individuals (CABHI) grant project\n\n\n point-person/project manager for follow up and grant work-plan compliance with Skid Row Community Consortium\n\n\n researched and identified future survey instruments for targeted sub-populations to improve data collection practices\n\n\n participated in regular trainings, program evaluation, and program development\n\n\n designed and edited Peer Advocate zine for commercial printing\n\n\n improved client access to resources/information through small booklets tailored to the population\n\n\n\n\n\n\n\n\n designed and implemented new digital signage/print fliers to promote library outreach, services, and events\n\n\n created new social media presence for (then) new platform Vine to promote library collections and services\n\n\n designed online content that highlighted collections, incorporated teaching and learning services and provided wide audience appreciation\n\n\n facilitated presentation on approach and ideas to UCLA Library‚Äôs social media steering committee\n\n\n operated multi-tiered social media platform apparatus\n\n\n\n\n\n\n\n\n\n\n\n Researched and implemented a suite of validated research questionnaires to collect survey data, deployed to database for end users to input; tidied, transformed, and modeled over 900 responses; and built and hosted a Shiny dashboard to communicate results. To be a part of the process from beginning to end was a huge learning process and I see it as a great accomplishment.\n\n\n Designed, proofed, prepared, and labored over Peer Advocate ‚ÄôZine for Skid Row Housing Trust‚Äôs Hilton Project ‚ÄôZine ‚ÄúI Got You‚Äù, which will be released during a end of grant event for all project participants\n\n\n Was asked to help design and brainstorm a flier for the UCLA Library system because of interest in design and outreach for libraries‚Äìthe final project was the ‚ÄúTop 10 Things‚Äù flier, which was a successful campaign to engage students in library programming and services\n\n\n Was recognized by two articles online for work promoting library services and collections through early-adoption of Vine\n\n\n\n\n\n Learning more about statistics and data science; particularly its applications in R. I am more and more becoming interested in Bayesian approaches.\n\n\n Coding: I will likely never code as neatly as an engineer but I try to incorporate software engineering best practices as I can‚ÄìI am very keen to become more proficient with workflows in airflow, deploying APIs to AWS, and taking on more data engineering as practicable.\n\n\n Design: I love typefaces and modernist design, in another life I would love to work as a designer or artist so it is something I passionately admire and sometimes try to do when time permits.\n\n\n Exploration: I like to explore other places through travel and food; ideologies through reading and communicating, and world through science and discovery!\n\n\nUpdated on: 2022-07-22"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Romuald ZAMI",
    "section": "",
    "text": "Cours de visualisation des donn√©es\n\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\nProfs programmation R - STID VCOD\n\n\n\n\n\n\n\n\nCours de manipulation des donn√©es\n\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2022\n\n\nProfs programmation R\n\n\n\n\n\n\n\n\nInteractive Dashboards with shiny\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2018\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nThoughts and Perspectives on PSH from the Ground Up\n\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2018\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nEx-nihilo Data Analysis; Getting up and Running with Limited Resources\n\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2017\n\n\nRobert Mitchell\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/2020-04-covid19-dashboard/index.html",
    "href": "projects/2020-04-covid19-dashboard/index.html",
    "title": "LA County COVID-19 Dashboard",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "talks/2017-09-Data-and-Donuts/index.html",
    "href": "talks/2017-09-Data-and-Donuts/index.html",
    "title": "Ex-nihilo Data Analysis; Getting up and Running with Limited Resources",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "talks/2018-12-east-la-r-users/index.html",
    "href": "talks/2018-12-east-la-r-users/index.html",
    "title": "Interactive Dashboards with shiny",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "talks/2018-09-ucla-cpi/index.html",
    "href": "talks/2018-09-ucla-cpi/index.html",
    "title": "Thoughts and Perspectives on PSH from the Ground Up",
    "section": "",
    "text": "Full Screen"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Romuald ZAMI",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "blog/2016-09-building-this-site-with-rmd/building-site-with-rmarkdown.html",
    "href": "blog/2016-09-building-this-site-with-rmd/building-site-with-rmarkdown.html",
    "title": "Building this site with RStudio and rmarkdown",
    "section": "",
    "text": "Important\n\n\n\nThis post is outdated; it would still work if you want to go this route, but the current site is built using Quarto and I think it‚Äôs a HUGE improvement"
  },
  {
    "objectID": "blog/2016-09-building-this-site-with-rmd/building-site-with-rmarkdown.html#things-that-werent-initially-clear",
    "href": "blog/2016-09-building-this-site-with-rmd/building-site-with-rmarkdown.html#things-that-werent-initially-clear",
    "title": "Building this site with RStudio and rmarkdown",
    "section": "Things that weren‚Äôt initially clear",
    "text": "Things that weren‚Äôt initially clear\n\nA few things I had to figure out on my own, which I‚Äôll share to make things easier.\n\n\nFooter background color does not extend across the page\n\nThis was tricky and what worked for me was looking at the rmarkdown site‚Äôs source code and including two closing <div>‚Äôs‚Äîhere‚Äôs how I worked it out in HTML and CSS:\n\nfooter-disqus.html\n\n  </div> <!-- articleBandContent -->\n</div> <!-- pageContent -->\n\n<footer>\n  <div id=\"rbmvFooter\" class=\"footer\">\n    <div class=\"footerContent\">\n    \n    <!-- footer content -->\n    \n    </div>\n  </div>\n  \n  <!-- js functions -->\n\n</footer>\n\n<!-- disqus js for comments -->\n\n\nThis is the CSS I used with the footer above:\n\nstyle.css\n#rbmvFooter {\n  position: relative;\n  z-index: 2;\n  box-sizing: border-box;\n  height: 25em;\n}\n\n#rbmvFooter.footer {\n  background-color: #212121;  /* light black */\n  padding: 4% 8%;\n  margin-top: 7em;\n  box-shadow: 0px 3px 50px 0px rgba(43,45,66,0.4);\n  font-weight: 300;\n  font-size: 13px;\n  line-height: 25px;\n}\n\n#rbmvFooter .footer {\n  color: #626262;  /* darker grey */\n  display: flex;\n  justify-content: space-between;\n  align-items: center;\n  line-height: 14px;\n}\n\n#scrollTop {\n  color: #FFFFFF!important; /* white */\n}\n\n\n\nSummarising content for the home page\n\nSo this is probably the most painful thing you‚Äôll have to do but it isn‚Äôt so bad! Once a post is finished, you‚Äôll have to add the HTML to the index.Rmd file and link to the page. Tools like Jekyll and Pelican handle a lot of this stuff automatically, but it isn‚Äôt too bad once you have an idea of how you want it to look. This is what I do:\n\n\nFinish blog post.\nAdd the post and summary / date info to the blog.Rmd file.\nCopy that HTML into the index.Rmd file with the three most up-to-date blog posts.\n\n\nI find it easiest to leave a template in comments that I can just fill in the blanks from the blog post I‚Äôve finished. I don‚Äôt think this is too much work. Also, RStudio may come up with some new tooling soon, you never know!\n\nblog.Rmd\n\n<!---\nentries will follow this format:\n<article>\n<h3><a href=\"\"> </a></h3>\n<div class=\"summary\">\n<span><i class=\"fa fa-calendar\"></i> _date_</span>\n<p> ...</p>\n<a class=\"btn btn-outline-primary btn-sm\" href=\"\">Full Post</a>\n</div>\n</article>\n--->\n\n<article>\n<h3><a href=\"blog-the-move-to-r.html\">THE MOVE TO R</a></h3>\n<div class=\"summary\">\n<span><i class=\"fa fa-calendar\"></i> _2016-08-29_</span>\n<p>This is _not_ a language wars type post.  I do not think there is some Mordor forged language to _rule them all_.  I debated whether or not to even write a post like this.  Nevertheless, I had a chance to meet and talk to [Jake Powray](https://twitter.com/jakeporway) from [DataKind](http://www.datakind.org) at the [DoGoodData](http://www.dogooddata.com) conference and I...</p>\n<a class=\"btn btn-outline-primary btn-sm\" href=\"blog-the-move-to-r.html\">Full Post</a>\n</div>\n</article>\n\n\n\n\nAnchor tag in page links missed location\n\nOn my home page I use the smooth scrolling function to move between the summarized content with anchor links and the id attribute. Since I added the fixed class to my _navbar.html file, I forgot to adjust for those pixels. This was fixed with the following CSS:\n\nstyle.css\na.anchor {\n  display: block;\n  position: relative;\n  top: -45px; /* fixed navbar height */\n  visibility: hidden;\n}\n\n\n\nDisplaying CSS in a code chunk\n\nTo get CSS to display in this blog post I had to git checkout gh-pages in the clone of rmarkdown that I made (which I recommend!) to see what they did to display CSS. I mostly use the following format for code chunks: {r, eval=FALSE}, {html, eval=FALSE}, {python, eval=FALSE}, or any of the other engines knitr can handle. {css, eval=FALSE} wouldn‚Äôt render for me when writing this post. Instead I had to use the following:\n\n```css\n.someClass {\n  color: #FFFF00;\n}\n```\n\n\n\n\nAlso, how do you put a code chunk in a code chunk?\n\n\n\nSorry, I had to do that. Trying to render the above CSS chunk proved harder than I thought. It lead me to try and figure out how to put a code chunk in a code chunk (and I thought of the ‚Äòyo dawg‚Äô meme right away). Boosted strait from RStudio‚Äôs site, this is how you accomplish it if you ever need to know:\n\n\n<pre class=\"markdown\"><code>&#96;&#96;&#96;css\n.someClass {\n  color: #FFFF00;\n}\n&#96;&#96;&#96;\n</code></pre>\n\n\n\n\nGetting Disqus to work\n\nFirst you‚Äôll need to set up an account with Disqus. The instructions are pretty self expanatory for installing Disqus on your page. Once you‚Äôve followed all the instructions, you should be able to navigate to the ‚ÄòInstallation‚Äô tab, which presents you with options for installing. You‚Äôll need to select ‚ÄúI don‚Äôt see my platform listed, install manually with Universal Code‚Äù. You‚Äôll be presented with a Jacascript function to place in your site.\n\nI initally didn‚Äôt follow Disqus instructions and tried to make it work the way RStudio had it set up in their footer but couldn‚Äôt get it to work‚Äîwhat I did was first add the script tag that gives you a count of comments (step 1 under the ‚ÄòHow to display comment count‚Äô) and then add just the function right after (step 1). If you look at the commented out section in the function, you‚Äôll see that Disqus wants you to set two variables and uncomment, but I couldn‚Äôt seem to get them to work with my site set up the way it is. If anyone has any insight into how the variables could be set up with an rmarkdown website, let me know in the comments!\n\n\n<!-- disqus -->\n<script id=\"dsq-count-scr\" src=\"//rbmv.disqus.com/count.js\" async></script>\n<div id=\"disqus_thread\" class=\"disqusPadding\"></div>\n  <script>\n    (function() { // DON'T EDIT BELOW THIS LINE\n      var d = document, s = d.createElement('script');\n      s.src = '//rbmv.disqus.com/embed.js';\n      s.setAttribute('data-timestamp', +new Date());\n      (d.head || d.body).appendChild(s);\n    })();\n  </script>\n  <noscript>Please enable JavaScript to view the <a href=\"https://disqus.com/?ref_noscript\">comments powered by Disqus.</a></noscript>\n\n\n\n\nSyntax highlighting\n\nI‚Äôm not a big fan of the options we inherit from Pandoc. Also, the CSS generated from code chunks seems kind of arbitrary and doesn‚Äôt work with Pygments (which is a shame because there are many Pygments CSS files to be found online!). From what I can tell here the markup classes come from the highlighting-kate package. When I looked at the Solarized webpage, there‚Äôs a link to the pages css that gave me a bit of a head start. I tweaked it a bit to try and make the chunks look like the syntax highlighting in R, but it isn‚Äôt perfect. If you would like to use my highlight theme, just copy this into your CSS:\n\nstyle.css\n\nroot: {\n  --base03: #002b36;  /* background */\n  --base02: #073642;  /* other background */\n  --base01: #586e75;  /* comments / secondary content */\n  --base1: #93a1a1;  /* body text / default code / primary content */\n  --yellow: #b58900;  /* keyword */\n  --orange: #cb4b16;  /* constants */\n  --red: #dc322f;  /* regex, special keywords */\n  --magenta: #d33682;  /* not sure what to do with this color yet */\n  --violet: #6c71c4;  /* not sure what to do with this color yet */\n  --blue: #268bd2;  /* reserved keywords */\n  --cyan: #2aa198;  /* strings, numbers */\n  --green: #859900;  /* operators, other keywords */\n}\n\n/*\n * Solarized Dark \n * http://ethanschoonover.com/solarized\n */\n\npre, code { font-family: 'Roboto Mono', monospace ; font-weight: 300; }\npre { background-color: var(--base03); color: var(--base1); }\ncode.sourceCode .kw { color: var(--blue); font-weight: 700; } /* Keyword */\ncode.sourceCode .dt { color: var(--cyan); } /* DataType */\ncode.sourceCode .dv { color: var(--base01); } /* DecVal */\ncode.sourceCode .bn { color: var(--orange); } /* BaseN */\ncode.sourceCode .fl { color: var(--cyan); } /* Float */\ncode.sourceCode .ch { color: var(--red); } /* Char */\ncode.sourceCode .st { color: var(--green); } /* String */\ncode.sourceCode .co { color: var(--base01); font-style: 100i; } /* Comment */\ncode.sourceCode .ot { color: var(--base1); } /* Other */\ncode.sourceCode .al { color: var(--green); font-weight: 700; } /* Alert */\ncode.sourceCode .fu { color: var(--blue); } /* Function */\ncode.sourceCode .er { color: var(--red); font-weight: 700; } /* Error */\ncode.sourceCode .wa { color: var(--base1); font-weight: 700i; } /* Warning */\ncode.sourceCode .cn { color: var(--orange); } /* Constant */\ncode.sourceCode .sc { color: var(--red); } /* SpecialChar */\ncode.sourceCode .vs { color: var(--cyan); } /* VerbatimString */\ncode.sourceCode .ss { color: var(--cyan); } /* SpecialString */\ncode.sourceCode .im { color: var(--red); } /* Import */\ncode.sourceCode .va { color: var(--blue); } /* Variable */\ncode.sourceCode .cf { color: var(--green); font-weight: 700; } /* ControlFlow */\ncode.sourceCode .op { color: var(--yellow); } /* Operator */\ncode.sourceCode .bu { color: var(--blue); } /* BuiltIn */\ncode.sourceCode .ex { } /* Extension */\n\n\n\nI rand the code chunk above as {html, eval=FALSE} because some of the CSS variables were being classified as .er, which looked really off. I‚Äôm still working on the best solution for the syntax highlighting.\n\n\nYou can also go crazy with this and create your own theme that mimics whatever color scheme you are going to use for your site. I thought these colors looked good with my site‚Äôs color scheme and the colors I plan to use for data visualizations are still in the works‚ÄîI will likely change them pretty often.\n\n\n\nPut some ggplot2 in a post!\n\nThe most rewarding part about writing blog posts in rmarkdown is that you can really dive into an analysis workflow and it is automagically a blog post. It‚Äôs so easy‚Äîyou just leave your code chunk as is or pass eval = F if you just want to show some code8 or echo = F to hide or display code the way you want to.\n\n\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(dplyr))\n\n# these are the colors I wanted to use for the site; may not be good for viz\nrbmv_palette <- c(\"#BFF073\", \"#0DC9F7\", \"#7F7F7F\", \"#F05B47\", \"#ED1C24\")\n\nmtcars %>%\n  ggplot(aes(x = disp, y = mpg)) +\n    geom_point(color = rbmv_palette[3]) + \n    geom_smooth(formula = y ~ x, se = F, method = \"loess\", \n                color = rbmv_palette[2])\n\n\n\n\n\n\nWhen using ggplot in this way, it creates a directory with an image file to use when creating the HTML. One of the reasons its easier to just use Plotly for web/interactive type stuff.\n\n\n\n\nOk, but how do I add plotly to a page?\n\nI‚Äôve been really into Plotly lately and one of the first things I wanted to do was add it to my website. There are a couple of hoops you have to jump through but luckily there is the wonderful htmltools package to help out9. Oh, and pipes. Plotly loves %>%‚Äôs. I‚Äôll just redo the plot above so you get an idea.\n\n\nsuppressPackageStartupMessages(library(plotly))\n\nrbmv_palette <- c(\"#BFF073\", \"#0DC9F7\", \"#7F7F7F\", \"#F05B47\", \"#ED1C24\")\n\nmtcars <- mtcars[order(mtcars$disp), ]\n\nmtcars %>% plot_ly(x = ~disp, y = ~mpg, \n                   type = \"scatter\", mode = \"markers\",\n                   text = ~paste(rownames(mtcars), \"<br>Mpg: \", \n                                 mpg, \"Disp: \", disp), \n                   showlegend = FALSE,\n                   colors = rbmv_palette) %>%\n  add_lines(x = ~disp, y = ~fitted(loess(mpg ~ disp)), \n            mode = \"lines\", name = \"Loess Smoother\", \n            showlegend = TRUE, \n            line = list(color = rbmv_palette[2])) \n\n\n\n\n\n\n\n\nGitHub Pages\n\nYou really don‚Äôt need to do a whole lot to host on GitHub. GitHub has really helpful instructions where you can see the two kinds of GitHub pages: a user or organizational account (which is what you‚Äôll use for your blog), or a project page. The only other thing you‚Äôll need to do is creat a .nojekyll file in your repository which you can do quickly in Bash:\n\n\ntouch .nojeckyll\n\n\nIf you‚Äôve purchased a domain and want to use it, you‚Äôll need a CNAME file in your directory as well. This is a really easy file to create:\n\nCNAME\n\nvim CNAME\n\n\nYou are now in Vim, a text editor, and if you‚Äôve never been here will be really confused! So just follow these steps:\n\nPress the i key to ‚Äòinsert‚Äô, or, type into the text editor.\ntype your url into the file, in my case:\n\n\nrobertmitchellv.com\n\n\nHit the esc key to exit ‚Äòinsert‚Äô mode\nType the : key to enter a command (you enter the command at the bottom of the terminal window)\nType ‚Äòwq‚Äô and then hit return\n\nOk, that was sneaky, you can also just create a new file in RStudio (text file) and enter it that way and save it. That will be much easier!\n\n\n\nThat‚Äôs it!\n\nThat‚Äôs really all I‚Äôve done so far. I just rebuilt this site last week and I‚Äôm likely to learn a lot more as I continue but wanted to share what I‚Äôve learned for anyone interested."
  },
  {
    "objectID": "blog/2017-12-adv-of-code-day-one/adv-of-code-day-one.html",
    "href": "blog/2017-12-adv-of-code-day-one/adv-of-code-day-one.html",
    "title": "Advent of code in R: day one",
    "section": "",
    "text": "Over on the rOpenSci Slack, Sam asked if anyone was doing the Advent of Code challenges in R. A few others said they were interested and I decided to go for it as well! My solutions are likely not as savvy as the other more experienced programmers, but it was a fun way to see how other people approach problems and if there is anything about their approach that you can incorporate into your programming style.\nI tend to work often with tibbles and rely often on dplyr so my solution orients itself around a dataframe and using common dplyr functions to solve the problem."
  },
  {
    "objectID": "blog/2017-12-adv-of-code-day-one/adv-of-code-day-one.html#the-problem",
    "href": "blog/2017-12-adv-of-code-day-one/adv-of-code-day-one.html#the-problem",
    "title": "Advent of code in R: day one",
    "section": "The problem",
    "text": "The problem\nThere are two parts to this problem:\n\nPart 1\nThe captcha requires you to review a sequence of digits (your puzzle input) and find the sum of all digits that match the next digit in the list. The list is circular, so the digit after the last digit is the first digit in the list.\nFor example:\n\n1122 produces a sum of 3 (1 + 2) because the first digit (1) matches the second digit and the third digit (2) matches the fourth digit.\n1111 produces 4 because each digit (all 1) matches the next.\n1234 produces 0 because no digit matches the next.\n91212129 produces 9 because the only digit that matches the next one is the last digit, 9.\n\n\nYou‚Äôre given data in the form of a long string of integers and task to solve the problem by providing the sum of the digits provided based on the rules introduced above.\n\nHere are the packages I used\nLoad Packages\n\nlibrary(tidyverse)\n\n\nMy puzzle input\n\n\ndata <- c(\"6592822488931338589815525425236818285229555616392928433262436847386544514648645288129834834862363847542262953164877694234514375164927616649264122487182321437459646851966649732474925353281699895326824852555747127547527163197544539468632369858413232684269835288817735678173986264554586412678364433327621627496939956645283712453265255261565511586373551439198276373843771249563722914847255524452675842558622845416218195374459386785618255129831539984559644185369543662821311686162137672168266152494656448824719791398797359326412235723234585539515385352426579831251943911197862994974133738196775618715739412713224837531544346114877971977411275354168752719858889347588136787894798476123335894514342411742111135337286449968879251481449757294167363867119927811513529711239534914119292833111624483472466781475951494348516125474142532923858941279569675445694654355314925386833175795464912974865287564866767924677333599828829875283753669783176288899797691713766199641716546284841387455733132519649365113182432238477673375234793394595435816924453585513973119548841577126141962776649294322189695375451743747581241922657947182232454611837512564776273929815169367899818698892234618847815155578736875295629917247977658723868641411493551796998791839776335793682643551875947346347344695869874564432566956882395424267187552799458352121248147371938943799995158617871393289534789214852747976587432857675156884837634687257363975437535621197887877326295229195663235129213398178282549432599455965759999159247295857366485345759516622427833518837458236123723353817444545271644684925297477149298484753858863551357266259935298184325926848958828192317538375317946457985874965434486829387647425222952585293626473351211161684297351932771462665621764392833122236577353669215833721772482863775629244619639234636853267934895783891823877845198326665728659328729472456175285229681244974389248235457688922179237895954959228638193933854787917647154837695422429184757725387589969781672596568421191236374563718951738499591454571728641951699981615249635314789251239677393251756396\")\n\n\nIt may seem weird, but I thought it would be easier to read this in as a string and then use stringr::str_split() to get each value separated in order to aid in processing.\nSplit the data into a single digit vector\n\ndigits <- data %>% str_split(\"\")\n\n\nMy next idea was to create an index so I don‚Äôt have to use a for() loop and rely on the index generated from that process. I also converted the digits back to the integer data type.\nConvert vector to a tibble and add an index\n\npuzzle <- tibble(digits = digits[[1]]) %>% \n  mutate(\n    index = row_number(),\n    digits = parse_integer(digits)) %>% \n  select(index, digits)\n\n\nI find setting up even really simple if_else logic so much easier when using case_when() since I don‚Äôt have to worry about the dataframe and can use the variable name. We‚Äôre just checking to see if the digit ahead is similar to the digit before and designating the flag in another column with either Match or No Match.\nFind out where the matches are\n\npuzzle <- puzzle %>%\n  mutate(match = case_when(\n    digits == digits[index + 1] ~ \"Match\",\n    TRUE ~ \"No Match\")) \n\nI was convinced for a second I could do something like puzzle$digits[-1] to get the last value but then remembered I was thinking about Python and not R‚Äìwhoops! Here I‚Äôm just checking to see if the first and last digits match since the list is conceptually circular. This reminds me of the first and last lines of Finnegan‚Äôs Wake being circular. In any case, this is just a quick check.\nCheck if last and first digits match\n\npuzzle$match[1] <- if_else(puzzle$digits[1] == puzzle$digits[length(puzzle$digits)], \"Match\", \"No Match\")\n\n\nNow we can get the sum and check our work to see if our solution returned a correct response.\nGet the sum\n\npuzzle %>%\n  filter(match == \"Match\") %>%\n  summarise(sum_of_matches = sum(digits))\n\n# A tibble: 1 x 1\n  sum_of_matches\n           <int>\n1           1029\n\n\n\nYAY Correct! ü•Ç\n\n\n\nPart 2\n\nHere are the rules for part two:\nNow, instead of considering the next digit, it wants you to consider the digit halfway around the circular list. That is, if your list contains 10 items, only include a digit in your sum if the digit 10/2 = 5 steps forward matches it. Fortunately, your list has an even number of elements.\nFor example:\n\n1212 produces 6: the list contains 4 items, and all four digits match the digit 2 items ahead.\n1221 produces 0, because every comparison is between a 1 and a 2.\n123425 produces 4, because both 2s match each other, but no other digit has a match.\n123123 produces 12.\n12131415 produces 4.\n\n\nMy initial thought is to just break the dataframe in half and then check if the digits match, which is acomplished by sliceing it in half and preparing to bind the columns by renaming some variables.\n\nfirst_half <- puzzle %>%\n  slice(1:(nrow(puzzle) / 2)) %>%\n  select(-match) %>%\n  rename(\n    first_index = index,\n    first_digits = digits)\n\nsecond_half <- puzzle %>%\n  slice(((nrow(puzzle) / 2) + 1):nrow(puzzle)) %>%\n  select(-match) %>%\n  rename(\n    second_index = index,\n    second_digits = digits)\n\n\nNow it is a simple bind_cols and then checking for matches, adding matches together and summing that column to get our answer.\n\nfirst_half %>% bind_cols(second_half) %>%\n  mutate(match = if_else(first_digits == second_digits, \"Match\", \"No Match\")) %>%\n  filter(match == \"Match\") %>%\n  mutate(total = first_digits + second_digits) %>%\n  summarise(sum = sum(total))\n\n# A tibble: 1 x 1\n    sum\n  <int>\n1  1220\n\n\nYES! Correct again! üéâ"
  },
  {
    "objectID": "blog/2015-01-first-kaggle-submission/first-kaggle-submission.html",
    "href": "blog/2015-01-first-kaggle-submission/first-kaggle-submission.html",
    "title": "First Kaggle Submission‚ÄìRandom Forest Classifier",
    "section": "",
    "text": "I have seen kaggle mentioned on twitter a lot; mostly by the data scientists and researchers I look up to, but there‚Äôs never been much confidence that the site was for me in any way‚Äîmostly because I was a long way from my dream data science job with yet so much to learn. Notwithstanding, I cannot help but try and hack my way to my destination! I think it‚Äôs a part of my learning process: thrust myself in the midst of something I don‚Äôt understand, get stuck, try to get unstuck, finish with some understanding of what I was doing.\n\nSo, when I saw this post by Chris Clark, I thought that it was about time I try and hack my way from recently learning Python to machine learning with SciKit-Learn&‚Äîwhy not!?&‚ÄîI thought.\n\nIt reminded me of when I decided to sign up with an account at GitHub; I was initially intimidated because it was new to me. Now, I use git in the command line, host my website there, and use it for almost everything (still learning new things about git everyday as well).\n\nChris‚Äôs post was excellent but there was one problem: the code was aimed at Python 2.7 users and I had just spent the previous semester learning Python 3 (which means I don‚Äôt really know 2.7; and avoid it all the time ‚Äúwhere are the parens for this print statement??‚Äù). As a personal challenge, I decided to use the code and update it to Python 3, which was both fun and challenging (I‚Äôm measuring ‚Äòupdate‚Äô to mean, ‚Äòrunning in my Python 3.4 interpreter without error messages‚Äô). This may be an easy task but there were a few snags for me.\n\nIn the spirit of trying to document the things I learn, I‚Äôve decided to chronical my results here&‚Äîif there are any errors or issues with this code, please let me know so I can try to correct, learn, and grow! I also found Chris‚Äôs updated code on GitHub, which uses Pandas and I‚Äôve been trying to get started with Pandas as well so; win, win.\n\nAs an aside, I use Anaconda and Vim for the enviornment and editing, respectively. My code can be found on GitHub.\n\nThe Submission was a part of the Predicting a Biological Response competition, and the training, test, and benchmark data sets are provided.\n\nSince the competition wants us to predict binary values, Chris notes that this data set is a good introduction to ensemble classifiers, because the prediction is a binary value (0 or 1). It was also great to take a closer look at both the Pandas and SciKit-Learn‚Äôs documentation to troubleshoot. I tried to use the comments to explain as much as possible so future me will not be baffled, which I can say is helpful since I‚Äôm looking at this one month out and it makes total sense (at least to me).\n\n\n### Kaggle Submission Code\n\"\"\"\n    //kaggle submission\n    //Biological Response\n    --> random forest classifier\n\n    Author: Robertmitchellv\n    Date: Dec 16, 2104\n    Revised: Dec 22, 2014\n\"\"\"\n\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\ndef main():\n    # create the training + test sets\n    try:\n        data = pd.read_csv('Data/train.csv')\n    except IOError:\n        print(\"io ERROR-->Could not locate file.\")\n\n    target = data.Activity.values\n\n    train = data.drop('Activity', axis = 1).values\n\n    test = pd.read_csv('Data/test.csv').values\n\n    # create and train the random forest and call it 'rf'\n    # --> n_estimators = the number of trees in this forest, viz.\n    #     100 trees of forest\n    # --> n_jobs set to -1 will use the number of cores present on your system.\n    rf = RandomForestClassifier(n_estimators = 100, n_jobs = -1)\n    # fit(X, y[, sample_weight]) = build a forest of tress from the\n    # training set (X, y)\n    rf.fit(train, target)\n\n    # predict_proba(X) predict class probabilities for X as list\n    predicted_probs = [x[1] for x in rf.predict_proba(test)]\n\n    # prep data for use in pd.Series\n    molID, predictProbs = prepData(predicted_probs)\n\n    # use a dictionary with keys as col headers and values as lists pulled from\n    # previous prep function\n    df = {'MoleculeID': molID, 'PredictedProbability': predictProbs}\n\n    # pandas DataFrame = a tabular datastructure like a SQL table\n    predicted_probs = pd.DataFrame(df)\n\n    # write predicted_probs to file with pandas method .to_csv()--add header\n    # for submission\n    try:\n        predicted_probs.to_csv('Data/submission.csv', index = False)\n        print(\"File successfully written; check 'Data' folder\")\n    except IOError:\n        print(\"io ERROR-->Could not write data to file.\")\n\n# preparing data for conversion to pd.DataFrame\ndef prepData(alist):\n        # prepare list to be converted to pandas Series\n        colOne = []\n        colTwo = []\n        idx = 1\n\n        # for loop to set MoleculeID to match the benchmark;\n        # place values into list for easier wrangling as pd.Series\n        for i in alist:\n            colOne.append(idx)\n            colTwo.append(i)\n            idx += 1\n\n        return colOne, colTwo\n\n# call the main function\nmain()\n\n\nAfter performing this‚ÄìChris suggested to submit to kaggle; being an extra careful person by nature, I just had to perform the evaluation and cross validation first (I don‚Äôt know if any of you feel the same way). Unfortunately, I don‚Äôt really understand how the code works‚Äìthis is one of the problems when hacking through tutorials.\n\n\n### Evaluation/Logloss\n\"\"\"\n    //kaggle submission\n    //Biological Response\n    --> evaluation function (from Grunthus' post)\n\"\"\"\n\nimport scipy as sp\n\ndef logloss(act, pred):\n    \"\"\" Vectorised computation of logloss \"\"\"\n\n    #cap in official Kaggle implementation,\n    #per forums/t/1576/r-code-for-logloss\n    epsilon = 1e-15\n    pred = sp.maximum(epsilon, pred)\n    pred = sp.minimum(1-epsilon, pred)\n\n    #compute logloss function (vectorised)\n    ll = sum(   act*sp.log(pred) +\n                sp.subtract(1,act)*sp.log(sp.subtract(1,pred)))\n    ll = ll * -1.0/len(act)\n    return ll\n\n\nThe cross validation was trickier to understand, which I think is mostly due to my not really understanding what ensemble classifiers do, how the random forest classifier works, and more specifically; what training, test, and target data do within machine learning. This gave chase through the SciKit-Learn documentation and other resources online to get a better understanding of what the code was doing&‚Äîthere‚Äôs a lot to learn! The interesting aspect is how the SciKit-Learn reserves some actual data that it can test against the classifier‚Äôs predicted values. I tried to show in the comments how I was understanding what the code did at the time.\n\n\n### Cross Validation\n\"\"\"\n    //kaggle submission\n    //Biological Response\n    --> cross validation\n\"\"\"\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cross_validation import KFold\nimport numpy as np\nimport pandas as pd\nimport logloss\n\ndef main():\n    #read data from csv; use nparray to create the training + target sets\n    try:\n        train = pd.read_csv('Data/train.csv')\n    except IOError:\n        print(\"io ERROR-->Could not locate file.\")\n\n    target = np.array([x[0] for x in train])\n    train = np.array([x[1:] for x in train])\n\n    # in this case we'll use a random forest, but this could be any classifier\n    model = RandomForestClassifier(n_estimators = 100, n_jobs = -1)\n\n    # simple K-Fold cross validation. 10 folds.\n    cv = KFold(n = len(train), n_folds = 10, indices = False)\n\n    #iterate through the training and test cross validation segments and\n    #run the classifier on each one, aggregating the results into a list\n    results = []\n    for traincv, testcv in cv:\n        prob = model.fit(train[traincv], target[traincv]).predict_proba(train[testcv])\n        results.append(logloss.llfun(target[testcv], [x[1] for x in prob]))\n\n    #print out the mean of the cross-validated results\n    print('Results: ', str(np.array(results).mean()))\n\n# call main function\nmain()\n\n\nAfter I was able to execute the submission, logloss, and cross validation code without any errors, I submitted my code to kaggle. It was an exciting moment waiting to see what kind of score I would have recieved had I actually participated in the competition. I would have placed at 325 (well, I would have tied with another user for 325th); check out my results below.\n\n\n\nWell, that wraps up my first submission to kaggle. I really hope this is the first of many. Right now I‚Äôm working through the Think Stats + Think Bayes books to refresh my stats knowledge. I‚Äôm trying to find time to work on the Titanic tutorial through kaggle as well as perhaps throw a hat in the ring for Booz Hamilton‚Äôs Data Science Bowl. There‚Äôs so much to learn and I can‚Äôt wait for these concepts to become more natural and familiar."
  },
  {
    "objectID": "blog/2015-04-pandas-snippits/pandas-snippits.html",
    "href": "blog/2015-04-pandas-snippits/pandas-snippits.html",
    "title": "My pandas snippets‚Äìalways evolving",
    "section": "",
    "text": "The goal of this post is to keep me from googling pandas questions that I‚Äôve forgotten. I don‚Äôt know how many times I‚Äôve looked at the results and seen five or more StackOverflow links that have clearly already been clicked on; I feel like Sisyphus when this happens! So, here is what I‚Äôm currently committing to memory:\n\n\n### Make matplotlib.pyplot look better with no effort:\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\n\n### Delete column\ndel df['colName']\n\n\n### Rename columns\ndf.columns = ['col1', 'col2', 'col3'] # this does not reindex columns\n\n### Combine columns\ndf['newCol'] = df['col1'].map(str) + data['col2'] + data['col3'].astype('str')\n\n### Copy column\ndf['newCol'] = df['oldCol'] # where newCol is the copy\n\n\n### Reindex columns\ncols = ['col1', 'col2', 'col3', 'col4'] # list of how you'd like it\ndf = df.reindex(columns=cols)\n\n\n### Find out how many NaN values you have in a column\ndf['colName'].isnull().sum()\n\n\n### Show unique values\ndf[df['colName'].unique()]\n\n### Create a frequency column from another column\ndf['freq'] = df.groupby('colName')['colName'].transform('count')\n\n### Delete row\ndf = df.drop(2)  # where two is the df's index\ndf = df.drop('rowName')  # if you reindexed\n\n\n### Remove characters before a specific character\ndf['colName'] = df['colName'].apply(lambda x: x.split('-')[-1]) # char = -\n\n\n### Remove characters after a specific character\ndf['colName'] = df['colName'].apply(lambda x: x.split('-')[0]) # char = -\n\n\n### Remove characters, e.g., commas from data\ndf['colName'] = df['colName'].str.replace(',', '')\n\n\n### Convert datatypes, e.g., object to float\ndf[['col4', 'col5', 'col10']] = df[['col4', 'col5', col10]].astype(float)\n\n\n### Convert string date to datetime64\ndf['strDate'] = pd.to_datetime(df['strDate'])\n\n\n### Filter datetime64 column values\nimport datetime\ndf[df['colName'] >= datetime.date(2015, 1, 1)]\n\n\n### Convert NaN values to zeros (or anything else)\ndf = df.fillna(0) # remember that this returns a new object!\n\n\n### Replace string values with numeric representations\ndictionary = {'value1': 1, 'value2': 2, 'Value3': 3}\ndf = df.replace({'colName': dictionary})\n\n\n### Replace multiple cells of a column only with a different string\ndf.loc[df['colName'].str.contains('word'), df['colName']] = \"Different Word\" # or\ndf.loc[df['colA'].str.contains('word'), ['colB']] = 5 # to change a cell in a different column\n\n\n### Project data based on a value range from a column\ndf[df.colWithNumbers <= 360] # shows me values less than or equal to 360\ndf[df['colWithStrings'].str.contains(\"word\")] # shows me values with 'word' in them\n\n\n### Project data based on two values (use and or pipe symbol to denote relationship)\ndf[(df['colWithString'].str.contains(\"word\")) & (df.colWithNumber <= 5)] # and\ndf[(df['colWithString'].str.contains(\"firstWord\")) | (df['colWithString'].str.contains(\"secondWord\"))] # or\n\n\n### Groupby as variable\ngroupedby = df.groupby(df.colName) # or:\ngroupedby = df.groupby(df.colName).add_suffix('/Mean') # add column suffixes\n\n\n### Use groupedby variable and find the mean for your values\ngroupedbyMean = groupedby.mean()"
  },
  {
    "objectID": "blog/2016-08-the-move-to-r/the-move-to-r.html",
    "href": "blog/2016-08-the-move-to-r/the-move-to-r.html",
    "title": "The move to R",
    "section": "",
    "text": "This is not a language wars type post. I do not think there is some Mordor forged language to rule them all. I debated whether or not to even write a post like this. Nevertheless, I had a chance to meet and talk to Jake Powray from DataKind at the DoGoodData confrence and I mentioned my moving from Python to R, which he wanted to hear about since normally he hears about the movement going in the other direction. Since my direction is a bit atypical and since my use case is also different, I decided to write about it. If you have any thoughts feel free to comment below!\n\n\nIt‚Äôs been almost a year since I posted anything on this site and quite a bit has changed for me since then. I moved into a more formalized data analyst role at the nonprofit I work at, I developed and put together a survey that relies on around ten research instruments to use as psychometrics (with a longitudinal goal of updating every six months), built a tool to collect the data, tidied the initial 900 responses, performed some clustering that shows a lot of promise, built a Shiny dashboard to communicate results to stakeholders that is being used by programming staff to target their efforts as they work to meet the needs of our respondents, and am gearing up to start bootstrapping the first batch of six month follow-up data to see if any changes are significant. Looking back at my previous posts I am happy to report that I‚Äôve learned a ton‚Äîand much of it is due to my picking up and diving into R.\n\nThis isn‚Äôt to say that I couldn‚Äôt have learned what I did in Python‚Äîit just happened to work out in my case that R had a kind of pedagogical effect, which I‚Äôll talk about more in a bit. First, there is no argument that there are insanely smart people who build tools in both languages (some that even collaborate, e.g., feather). Thankfully, I‚Äôve never seen an argument that centers around this theme, i.e., that one language holds the corner on sharp programmers. I do, however, see arguments that appear to be preference masquerading as axiomatic, aphoristic, or general maxim. I‚Äôm not sharing some unseen truth; rather, I‚Äôm just describing what helped me move forward and gain a better understanding of the data analysis workflow that seems to work for me.\n\nA lot has been said about the steep learning curve of programming in R. By contrast, Python is one of the most popular introductory teaching languges in CS departments at universities across the US. I knew this before I made a choice of which language to learn when I became interested in data analysis. Based on that understanding, I took a Python 3 course at a community college but mostly lived off of the interactive version of How to Think Like A Computer Scientist. Like many people who become interested in data, there is a lot of ground to cover before beginning actual analysis. First, I needed to learn how to program. Second, I needed to learn about computer science. I found the interactive site to be the best way for me to actually get a feel for what happens when I run code and I found Python to be an excellent communicator of general CS ideas. After I finished my Python course and began understanding a bit more about what I was doing in Python I hit the ground running learning Pandas and matplotlib‚ÄîI thought I had closed the door on R. It didn‚Äôt make sense to stop progress in one language to begin all over again in another; especially when people like Hadley and Garret recommend that:\n\n\n However, we strongly believe that it‚Äôs best to master one tool at a time. You will get better faster if you dive deep, rather than spreading yourself thinly over many topics. This doesn‚Äôt mean you should only know one thing, just that you‚Äôll generally learn faster if you stick to one thing at a time. You should strive to learn new things throughout your career, but make sure your understanding is solid before you move on to the next interesting thing.1 \n\n\nI had not mastered Python when deciding to explore R. I have not mastered R either. Nevertheless, there was this nagging thought I couldn‚Äôt shake a while back when seeing language-wars-types of posts on twitter about leaving R. Namely, that one necessarily has to use R in order to leave it. Putting aside all the good and bad reasons I‚Äôve read for moving from R to Python, I wondered if R had some sort of pedagogical magic that would help me become a better ‚Äúdata scientist‚Äù like the ones I followed on twitter and respected a great deal. In my mind I figured that it must be a cross to bear in order to be a more well rounded data worker.\n\nSince I had spent considerable time figuring out how to annotate bar charts in matplotlib I figured this would be a good test for R: recreate the plots from that blog post to get a feel for what the language is like. To my chagrin, this was much, much, easier in ggplot2 than matplotlib. I felt like I struggled so hard to get matplotlib to give me a halfway decent looking plot while ggplot2 made my previous efforts seem futile. Even adding fit lines with one line of code was a crazy thought to me! I finally understood why Greg Lamp was working so hard to build ggplot in Python. I decided to spend my morning train rides learning more about R and about ggplot2. I didn‚Äôt know this at the time, but diving into ggplot2 and its beautiful API offered me insight I always found lacking when getting started with an analysis project in Python. The more I dove into the tidyverse the more I started to see how the API (especially dplyr) unfolds in such a way that a user is practically lead into an analysis workflow; a workflow that I found to be rewarding on many levels.2\n\nBecause of this rewarding relationship, I‚Äôve found a great partner in R‚Äîthis is especially due to my use case, which I mentioned above and will describe quickly before ending this post. I work as the only person within my org that writes code and works with data the way that I do. Because of this, I often am required to perform many tasks and be very flexible and accommodating of requests. R has proved a great workhorse in this regard; I am able to do so many different things in RStudio that work well for a person that must acomplish everything alone. I can take EDA from an Rmd_notebook, put it in a report, turn it into a webpage, or transform it into a Shiny dashboard. Heck, this whole website was written in RStudio and put together with rmarkdown::render_site(). I know there are many projects I‚Äôd like to work on where Python will be the tool of choice. However, at least for now I am dreadfully enamored with R. Python taught me how to think like a computer scientist, but R is teaching me how to think like a statistician. Pedagogically I feel that both languages are vital to growing as the person who works in this hodgepodge known as ‚ÄòData Science‚Äô.\n  \n\n\n\n\nFootnotes\n\n\nHadley Wickham and Garrett Grolemund, R For Data Science, http://r4ds.had.co.nz/introduction.html#python-julia-and-friends‚Ü©Ô∏é\nI know that Hadley has strong opinions about how an analysis workflow should unfold, e.g., ‚ÄúThere‚Äôs no reason that ifelse() couldn‚Äôt be made as fast as your proposed approach, and dplyr will never support in-place mutation of data. This is something I feel very strongly about. Can you provide more examples of the stata code you‚Äôre converting? In my experience, stata code tends to use a lot of nested if statements that I‚Äôd approach in a fundamentally different way in R.‚Äù Read discusion here‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/2015-06-bar-chart-annotations-pandas-mpl/bar-chart-annotations-pandas-mpl.html",
    "href": "blog/2015-06-bar-chart-annotations-pandas-mpl/bar-chart-annotations-pandas-mpl.html",
    "title": "Bar chart annotations with pandas and matplotlib",
    "section": "",
    "text": "When I first started using Pandas, I loved how much easier it was to stick a plot method on a DataFrame or Series to get a better sense of what was going on. However, I was not very impressed with what the plots looked like. Any time I wanted to do something slightly different from the ‚ÄúPlotting‚Äù documentation on the pydata site, I found myself arm deep in MPL code that did not make any damn sense to me. This was a problem for me, as I ended up spending way too much time trying to make small edits and not enough time working on the code I was trying to visualize.\n\nOne thing in particular bugged me. I could find no easy to understand tutorial on annotating a bar chart on StackOverflow or any other site. MPL had some documentation, but it was too confusing for me at the time. I spent a lot of time trying to figure out how to put some text right above my bars. Since I would have loved to see a snippet of code to help me in my journey, I thought I would throw it together in a brief post so others could use my workaround.\n\nI warn you, it is not the most elegent solution, I am sure, but it worked for me when I needed to demonstrate the insight I had gained from a Healthcare Access and Utilization Survey (made up mostly of CHIS questions) to people in my department, my director, and her bosses. Since I cannot share any of that data, I will use the War of the Five Kings Dataset that Chris Albon made. I love this data set because I am in the middle of book five of Game of Thrones, which provides a good amount of domain familiarity to enable jumping in easier.\n\n\nSetup + Import Data\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\n# set jupyter's max row display\npd.set_option('display.max_row', 1000)\n\n# set jupyter's max column width to 50\npd.set_option('display.max_columns', 50)\n\n# Load the dataset\ndata = pd.read_csv('site_content/data/5kings_battles_v1.csv')\n\n\n\n\nFirst visualization with annotations\n\n\nax = data['region'].value_counts().plot(kind='barh', figsize=(10,7),\n                                        color=\"coral\", fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"Where were the battles fought?\", fontsize=18)\nax.set_xlabel(\"Number of Battles\", fontsize=18);\nax.set_xticks([0, 5, 10, 15, 20])\n\n# create a list to collect the plt.patches data\ntotals = []\n\n# find the values and append to list\nfor i in ax.patches:\n    totals.append(i.get_width())\n\n# set individual bar lables using above list\ntotal = sum(totals)\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+.3, i.get_y()+.38, \\\n            str(round((i.get_width()/total)*100, 2))+'%', fontsize=15,\ncolor='dimgrey')\n\n# invert for largest on top \nax.invert_yaxis()\n\n\n\nThe image above is the output from the Jupyter notebook. I think it is super clear and gives a lot of information about where the battles were fought. However, I am very parital to horizontal bar charts, as I really think they are easier to read, however, I understand that a lot of people would rather see this chart implemented in a regular bar chart. So, here is the code to do that; you will notice that a few things have changed in order to create the annotation.\n\n\nax = data['region'].value_counts().plot(kind='bar', figsize=(10,7),\n                                        color=\"coral\", fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"Where were the battles fought?\", fontsize=18)\nax.set_ylabel(\"Number of Battles\", fontsize=18);\nax.set_yticks([0, 5, 10, 15, 20])\n\n# create a list to collect the plt.patches data\ntotals = []\n\n# find the values and append to list\nfor i in ax.patches:\n    totals.append(i.get_height())\n\n# set individual bar lables using above list\ntotal = sum(totals)\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()-.03, i.get_height()+.5, \\\n            str(round((i.get_height()/total)*100, 2))+'%', fontsize=15,\n                color='dimgrey')\n\n\n\n\nI play around with the mpl.text() numbers for almost each chart. They are never exactly where they need to be, which often means moving thigs around a hair here and .03 there. You can add or subtract, which means you can also do this:\n\n\nax = data['attacker_outcome'].value_counts().plot(kind='bar', figsize=(10,7),\n                                                  color=\"indigo\", fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"Do attackers usually win or loose?\", fontsize=18)\nax.set_ylabel(\"Number of Battles\", fontsize=18);\nax.set_yticks([0, 5, 10, 15, 20, 25, 30, 35, 40])\n\n# create a list to collect the plt.patches data\ntotals = []\n\n# find the values and append to list\nfor i in ax.patches:\n    totals.append(i.get_height())\n\n# set individual bar lables using above list\ntotal = sum(totals)\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()+.12, i.get_height()-3, \\\n            str(round((i.get_height()/total)*100, 2))+'%', fontsize=22,\n                color='white')\n\n\n\n\nIf you are like me, often you like to isolate a categorical value in one column and see what the rest of the dataframe looks like in light of that. It is a simply way of drilling down, but a percentage really would not be as appropriate as a count. Here is an example of using a count rather than a percentage:\n\n\nlosses = data[data['attacker_outcome'].str.contains(\"loss\", na=False)]\n\nax = losses['attacker_king'].value_counts().plot(kind='barh', figsize=(10,7),\n                                                 color=\"slateblue\", fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"Who were the attackers who lost?\", fontsize=18)\nax.set_xlabel(\"Number of Battles\", fontsize=18);\nax.set_xticks([0, 5])\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+.1, i.get_y()+.31, \\\n            str(round((i.get_width()), 2)), fontsize=15, color='dimgrey')\n\n# invert for largest on top \nax.invert_yaxis()\n\n\n\n\nYou can also just project a couple columns from those that lost to compare a couple of values; I think bar charts are great for this purpose. I am not sure what the best way would be do accomplish this, but here is my implementation:\n\n\nax = losses[['attacker_size', 'defender_size']].plot(kind='bar',\n              figsize=(15,7), color=['dodgerblue', 'slategray'], fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"For Attacker Losses, What was the Difference in Size?\",\nfontsize=18)\nax.set_ylabel(\"Number of Troops\", fontsize=18);\nax.set_yticks([0, 20000, 40000, 60000, 80000, 100000, 120000, 140000])\nax.set_xticklabels([\"Robb v Joff/Tommen\", \"Joff/Tommen v Robb\", \n                    \"Stannis v Joff/Tommen\", \"Robb v Joff/Tommen\", \n                    \"Stannis v Mance\"], rotation=0, fontsize=11)\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()+.04, i.get_height()+12000, \\\n            str(round((i.get_height()), 2)), fontsize=11, color='dimgrey',\n                rotation=45)\n\n\n\n\nThere is a handy ‚Äòrotation‚Äô option for the MPL plots that you can use that I feel works well when using a regular bar chart. I really dislike tilting my head to one side to try and read what it says! Also, it is easy to rename the columns! I did not realize how simple it was, which makes me feel silly.\n\nHere is the chart done horizontally, which I prefer:\n\n\nax = losses[['attacker_size', 'defender_size']].plot(kind='barh',\n              figsize=(10,7), color=['dodgerblue', 'slategray'], fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"For Attacker Losses, What was the Difference in Size?\",\nfontsize=18)\nax.set_xlabel(\"Number of Troops\", fontsize=18)\nax.set_ylabel(\"First Name is Attacker\", fontsize=18)\nax.set_xticks([0, 20000, 40000, 60000, 80000, 100000, 120000, 140000])\nax.set_yticklabels([\"Robb v Joff/Tommen\", \"Joff/Tommen v Robb\", \n                    \"Stannis v Joff/Tommen\", \"Robb v Joff/Tommen\", \n                    \"Stannis v Mance\"])\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+700, i.get_y()+.18, \\\n            str(round((i.get_width()), 2)), fontsize=11, color='dimgrey')\n\n# invert for largest on top \nax.invert_yaxis()\n\n\n\n\nI hope this is helpful for anyone out there trying to create little annotations for their visualizations. I feel like this is just a little bit of extra work but it keeps me from having to write JavaScript, which is worth a little copy paste action. When I have time, I would like to create a class with methods so I do not have to keep doing a copy/paste job in my Jupyter notebook.\n\nLet me know if there is an easier way to do this, I would be grateful!\nHere is a link to the notebook on my GitHub if you are interested in playing with it a bit more. I stopped when I was trying to figure out how to turn the dates into a Pandas ‚Äòperiod_range‚Äô."
  },
  {
    "objectID": "blog/2013-09-foucaults-challenge/foucaults-challenge.html",
    "href": "blog/2013-09-foucaults-challenge/foucaults-challenge.html",
    "title": "Foucault‚Äôs challenge to modernist classification",
    "section": "",
    "text": "In Foucault‚Äôs Les Mots et les choses (The Order of Things), he notes a passage in Borges that, for him, demonstrates the limitations of taxonomic assertions in the face of exotic systems of thought‚Äîvia Borges, he quotes a ‚Äòcertain Chinese encyclopedia‚Äô in which it is written:\n\n\n animals are divided into: (a) belonging to the Emperor, (b) embalmed, (c) tame, (d) sucking pigs, (e) sirens, (f) fabulous, (g) stray dogs, (h) included in the present classification, (i) frenzied, (j) innumerable, (k) drawn with a very fine camelhair brush, (l) et cetera, (m) having just broken the water pitcher, (n) that from a long way off look like flies1 \n\n\nWhat shocks Foucault about this passage is what connects these categories‚Äîi.e., the structure that links these strange juxtaposed oddities: the alphabetical series 2. Foucault questions on what bedrock would kinship between ‚Äò(i) frenzied, (j) innumerable, (k) drawn with a very fine camelhair brush‚Äô-animals meet, other than the non-space of the utterance itself; i.e., the non-space of language 3. Language can only display this kinship in an ‚Äòunthinkable space‚Äô (abstract locus)‚ÄîBorges wants to remove the ‚Äòoperating table‚Äô that enables thought to order, divide, and classify external entities 4. Effectively, this removes the ground upon which, ‚Äúsince the beginning of time, language has intersected space‚Äù 5. Borges‚Äô works often lies in the abstract space of the ‚Äòheterotopia‚Äô 6, which ‚Äúdesiccates speech, stops words in their tracks, contests the very possibility of grammar at its source; [‚Ä¶] [to] dissolve our myths and sterilize the lyricism of our sentences‚Äù 7. This highlights the challenge of classifying in the post-modern library, for now there is an unforeseen danger‚Äînot incongruous disorder but the linking together of things that are inappropriate 8.\n\nA perfect classification scheme that represents a universe of knowledge is the pipe-dream of the modernist; our observations are not independent of the external world, which undermines our ability to classify. We are enmeshed in our world; contextualized in the milieu that is the object of our analysis. We have no bird‚Äôs eye view‚Äîour objectivity has no locus from which to observe. Nevertheless, I feel that there is a space to enhance knowledge organization. First, I believe it is important to shed the illusion of a temporal permanence of facts, which is not solid and more fluid. Meaning, our reconstructions of the external world mirrored by our knowledge organization schemes change through time and are in flux. Second, I believe it is important to increase transparency and acknowledge bias which can exist through ethnocentrism, race, religion, gender, sex, power, language, geography, et cetera.\n\nEffectively this is similar to Jung‚Äôs notion of the shadow, that as humans we feel that it is silly to believe we cannot accurately describe the external world‚Äîwe all agree on things, we test them, and we derive data with which to harness confidence in talking objectively about the external world. We do this daily. It is this confidence that leaves us blind to the shadow of our foundation; like Venice, we are sinking. In order to bring ourselves back to a form of equilibrium we must admit that our shadow exists, namely, that we bring as much to our observations than we leave and, in light of this; we should attempt to root out future bias through honesty and self-understanding. In this way, we stand the chance of building more honest reflections of knowledge. And, again, in this way, we stand the chance of achieving some kind of universe of human knowledge, or at least a good representation of what we feel that we know.\n  \n\n\n\n\nFootnotes\n\n\nFoucault, M. 2002. The order of things an archaeology of the human sciences. London and New York: Routledge Classics (Original work published 1966).‚Ü©Ô∏é\nIbid‚Ü©Ô∏é\nIbid‚Ü©Ô∏é\nIbid‚Ü©Ô∏é\nIbid‚Ü©Ô∏é\n‚ÄúDifferent from utopias, which also have no locality, heterotopias are disturbing because they undermine language, make impossible the naming of ‚Äòthis‚Äô or ‚Äòthat‚Äô, because they shatter both the syntax that humans construct sentences with, as well as the syntax that holds words and things together‚Äù‚Ü©Ô∏é\nFoucault, 2002‚Ü©Ô∏é\nIbid‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/2015-10-vaccine-heatmap/vaccine-heatmap.html",
    "href": "blog/2015-10-vaccine-heatmap/vaccine-heatmap.html",
    "title": "Recreating the vaccine heatmap in plotly 4.0 with R",
    "section": "",
    "text": "This took me a while to figure out how to implement. Since plotly 4.0+ is so different1 and the documentation is still rolling out2 I wanted to challenge myself to make something complicated to better understand how things have changed. I think the Impact of Vaccines visualizations are some of the best examples of how powerful data visualization can be‚Äìit becomes difficult to mount an counter argument after seeing them. So, here goes!\n\nIn order to get the data for this visualization you will need to create a free account over at Project Tyco. You can download all the datasets if you‚Äôd like but I‚Äôm just going to use the Polio data set. These are the packages that I‚Äôm using:\n\nLoad Packages\n\nlibrary(tidyverse)\nlibrary(plotly)\n\n\nI should probably get in the habit of inspecting the csv files in vim before loading them so I can decide how I want to read in the file. In this case, using the readr package, you‚Äôll need to set a couple of options: first you‚Äôll need to set the skip argument to 2 since there are two lines of metadata about the data set. The second argument is na, which takes a simple vector for weirdly encoded NA values. In this data set, - is used for NA, so you‚Äôll set the na argument to c(\"-\", \"NA\") to ensure those values are handled correctly. The third arugment to set is col_types, which is an awesome way to control your data types during reading so you do less old-fashioned type conversion √† la df$x <- as.numeric(df$x), which I don‚Äôt think anyone likes doing. readr‚Äôs heuristic guesses correctly for all columns except the variables for ALASKA and HAWAII. I‚Äôm not exactly sure why, but my guess is that the 0.00 values mess with the NA checks that readr performs. Luckily, we don‚Äôt have to know the why since we can just change those two and leave readr to do its job on the others:\n\nRead in the data\n\npolio <- read_csv(\"POLIO_Incidence_1928-1969_20160904215505.csv\", \n                  skip = 2, na = c(\"-\", \"NA\"),\n                  col_types = list(\n                    ALASKA = col_double(),\n                    HAWAII = col_double()\n))\n\n\nNow, the way this data set is structures is super weird. In order to get it into a tidy format we‚Äôll need to use tidyr; specifically the gather function. But I don‚Äôt want to pass through all the variable names, so with a little lazy figuring out, I just kept changing the numbers until I found the right range in numbers with dplyr‚Äôs select function since it uses the same start_row:end_row notation:\n\nFind the index for the columns you want to gather with tidyr\n\npolio %>% select(3:53) %>% head()\n\n\n\n\n\n\n\n\n\nUse gather to transform data, rename columns, and make sure cases is numeric\n\npolio <- polio %>% gather(3:53, key=\"state\", value=\"cases\") \npolio <- rename(polio, year = YEAR, week = WEEK, state = state, cases = cases)\n\n\nThis is a quick function to get the states data in the right format to use R‚Äôs built in state.abb data, which I found online. I‚Äôve never used the match function before, but this seemed to work really well except for DC, which I‚Äôm using dplyr‚Äôs if_else function to correct. This is a sloppy function but it works for this data.\n\nFix all caps\n\nfix_state_names <- function(data) {\n  lower <- str_to_lower(data)\n  title <- str_to_title(lower)\n  alter <- state.abb[match(title, state.name)]\n  out <- if_else(is.na(alter), \"DC\", alter)\n  return(out)\n}\n\n\nApply the fix_state_names function to the state column\n\npolio$state <- map_chr(polio$state, fix_state_names)\n\n\nWe need to sum all the cases in a given year by state, but if the whole year contains NA values, I‚Äôm going to give it a value of 0.\n\ngroup_by to sum all the cases for a year and then ungroup data frame\n\npolio <- polio %>% \n  group_by(year, state) %>%\n  summarise(totals = if_else(all(is.na(cases)), 0, sum(cases, na.rm = T))) %>%\n  ungroup()\n\n`summarise()` has grouped output by 'year'. You can override using the `.groups`\nargument.\n\n\n\nSet margins for plot\n\nm <- list(\n  l = 40,\n  r = 50,\n  b = 50,\n  t = 50,\n  pad = 4\n)\n\n\nYou can create a custom colorscale in a lot of different ways, but I found creating a data frame with tribble to be the easiest way. Trying to replicate this colorscale was difficult because the blues, greens, and yellow colors are only representing values between 0-10% of the total values, so I had to keep changing things until it looked right. Using tribble helps because it is easier to line things up rather than counting vector locations with your finger on the screen (does anyone else do that!?).\n\nCreate custom colorscale\n\npolio_color <- tribble(\n  ~range, ~hex,\n  0.000,  \"#e7f0fa\",\n  0.025,  \"#c9e2f6\",\n  0.045,  \"#95cbee\",\n  0.065,  \"#0099dc\",\n  0.085,  \"#4ab04a\",\n  0.105,  \"#ffd73e\",\n  0.150,  \"#eec73a\",\n  0.300,  \"#e29421\",\n  0.450,  \"#f05336\",\n  1.000,  \"#ce472e\"\n)\n\n\nPlot the data\n\npolio %>%\n  plot_ly(\n    x = ~year, y = ~state,\n    height = 1000, width = 950) %>%\n  add_heatmap(\n    z = ~totals, zmin = 0, zmax = 190,\n    text = ~paste(\n      \"Year: \", year, \"<br>State: \", state,\n      \"<br>Total Cases: \", totals),\n    hoverinfo = \"text\", colorscale = polio_color, showscale = F,\n    opacity = 0.85) %>%\n  add_annotations(\n    \"Vaccine Introduced\", x = 1957.5, y = -1.5, showarrow = F) %>%\n  add_segments(\n    x = 1955, xend = 1955, y = ~state[3], yend = ~state[49],\n    line = list(width = 2, color = \"black\"),\n    name = \"1955\", hoverinfo = \"text\",\n    text = paste(\"Vaccine Introduced: 1955\")) %>%\n  layout(\n    title = \"Polio\",\n    xaxis = list(title = \"\", nticks = 10),\n    yaxis = list(title = \"\", autorange = \"reversed\"),\n    margin = m, autosize = F)\n\n\n\n\n\n\nThere‚Äôs a lot going on here, but the code is not too verbose (at least I don‚Äôt think). I add the year and state data to the regular plot_ly function while placing the z data that transforms it into a heatmap in add_heatmap. Even though I‚Äôm pushing the annotation outside of the plot, plotly doesn‚Äôt add any more y ticks. However, in add_segments if I used y = ~min(state), yend = ~max(state) there would have been two extra y ticks on the bottom (0 and 1). This is annoying and I tried many different things to get everything to behave nicely but it‚Äôs hard to get a heatmap and lines and annotations to work well together. My workaround is to just make the segment line a little shorter, but I don‚Äôt think it looks too bad.\n\nLet me know if you have any thoughts about improving things in the comments!\n  \n\n\n\n\nFootnotes\n\n\nhttp://moderndata.plot.ly/upgrading-to-plotly-4-0-and-above/‚Ü©Ô∏é\ngit clone and git pull often: https://cpsievert.github.io/plotly_book/‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "",
    "text": "After seeing many language wars style posts about  vs  and the sort of  to  comparisons being made, I realized that there aren‚Äôt many helpful side-by-sides that show you how to do x in y language (and vice versa), I thought about the kind of post I would like to see; one that leverages both tidyverse, modern pandas method-chaining / pyjanitor or polars, and plotly (in both R and Python).\nI decided to try and see if I could contribute something to the discourse. I‚Äôm not really trying to reinvent an analysis wheel and just want to focus on the how something is accomplished from one language to the other so I‚Äôm pulling from a few sources to just have some code to translate using the same data for both languages.\nSince polars is new to me and I like learning new things, I‚Äôm using it for the examples, but if you‚Äôre familiar with pandas already, I‚Äôd highly recommend pyjanitor."
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#data",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#data",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "Data",
    "text": "Data\n\nData was obtained from the dslab R package and written to parquet via R‚Äôs arrow::write_parquet for better interoporability between R and Python. Additionally, the size is low enough to pull the data as parquet from my GitHub repo."
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#fa-brands-r-project-packages",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#fa-brands-r-project-packages",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": " packages",
    "text": "packages\n\n\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(arrow, include.only = \"read_parquet\")\nlibrary(magrittr, include.only = \"%<>%\")\n\ngapminder <- read_parquet(\"gapminder.parquet\")"
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#fa-brands-python-libraries",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#fa-brands-python-libraries",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": " libraries",
    "text": "libraries\n\n\nimport polars as pl\nimport plotly.express as px\n\ngapminder = pl.read_parquet(\"gapminder.parquet\")\n\ngapminder = (gapminder\n  .with_columns([\n    pl.col(\"country\").cast(pl.Utf8),\n    pl.col(\"continent\").cast(pl.Utf8),\n    pl.col(\"region\").cast(pl.Utf8)  \n  ])\n)\n\n return top 10 rows in R\n\ngapminder %>% head(10)\n\n\n\n  \n\n\n\n get quick info on the data with dplyr::glimpse()\n\ngapminder %>% glimpse()\n\nRows: 10,545\nColumns: 9\n$ country          <fct> \"Albania\", \"Algeria\", \"Angola\", \"Antigua and Barbuda\"‚Ä¶\n$ year             <int> 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960, 1960,‚Ä¶\n$ infant_mortality <dbl> 115.40, 148.20, 208.00, NA, 59.87, NA, NA, 20.30, 37.‚Ä¶\n$ life_expectancy  <dbl> 62.87, 47.50, 35.98, 62.97, 65.39, 66.86, 65.66, 70.8‚Ä¶\n$ fertility        <dbl> 6.19, 7.65, 7.32, 4.43, 3.11, 4.55, 4.82, 3.45, 2.70,‚Ä¶\n$ population       <dbl> 1636054, 11124892, 5270844, 54681, 20619075, 1867396,‚Ä¶\n$ gdp              <dbl> NA, 13828152297, NA, NA, 108322326649, NA, NA, 966778‚Ä¶\n$ continent        <fct> Europe, Africa, Africa, Americas, Americas, Asia, Ame‚Ä¶\n$ region           <fct> Southern Europe, Northern Africa, Middle Africa, Cari‚Ä¶\n\n\n return top 10 rows in Python\n\ngapminder.head(10)\n\n\n\n\n\n  \n\n\n\n get quick info on the data with pandas‚Äôs info DataFrame method\n\ngapminder.to_pandas().info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10545 entries, 0 to 10544\nData columns (total 9 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   country           10545 non-null  object \n 1   year              10545 non-null  int32  \n 2   infant_mortality  9092 non-null   float64\n 3   life_expectancy   10545 non-null  float64\n 4   fertility         10358 non-null  float64\n 5   population        10360 non-null  float64\n 6   gdp               7573 non-null   float64\n 7   continent         10545 non-null  object \n 8   region            10545 non-null  object \ndtypes: float64(5), int32(1), object(3)\nmemory usage: 700.4+ KB\n\n\nThis will come back later, but it‚Äôs very easy to move your polars data into a pandas DataFrame."
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#hans-roslings-quiz",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#hans-roslings-quiz",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "Hans Rosling‚Äôs quiz",
    "text": "Hans Rosling‚Äôs quiz\n\nFollowing along Hans Rosling‚Äôs New Insights on Poverty video, we‚Äôre going to answer the questions he poses in connection to child mortality rates in 2015. He asks, which pairs do you think are most similar?\n\nSri Lanka or Turkey\nPoland or South Korea\nMalaysia or Russia\nPakistan or Vietnam\nThailand or South Africa\n\n\n\nSri Lanka or Turkey\n\n simple dplyr::filter and dplyr::select\n\ngapminder %>%\n  filter(year == \"2015\", country %in% c(\"Sri Lanka\", \"Turkey\")) %>%\n  select(country, infant_mortality)\n\n\n\n  \n\n\n\n simple filter and select method chain\n\n(gapminder\n  .filter(\n    (pl.col(\"year\") == 2015) & \n    (pl.col(\"country\").is_in([\"Sri Lanka\", \"Turkey\"]))) \n  .select([\"country\", \"infant_mortality\"])\n) \n\n\n\n\n\n  \n\n\n\nThis is where you can start to see how powerful polars can be in terms of the way it handles lazy evaluation. One of the reasons dplyr is so expressive and intuitive (at least in my view) is due in large part to the way it handles lazy evaluation. For people that are tired of constantly needing to refer to the data and column in pandas will likely rejoice at polars.col!\n\n\n\nLet‚Äôs just compare them all at once\n\n same strategy; more countries\n\ngapminder %>%\n  filter(\n    year == \"2015\", \n    country %in% c(\n      \"Sri Lanka\", \"Turkey\", \"Poland\", \"South Korea\",\n      \"Malaysia\", \"Russia\", \"Pakistan\", \"Vietnam\",\n      \"Thailand\", \"South Africa\")) %>%\n  select(country, infant_mortality) %>%\n  arrange(desc(infant_mortality))\n\n\n\n  \n\n\n\n same as above\n\n(gapminder\n  .filter(\n    (pl.col(\"year\") == 2015) & \n    (pl.col(\"country\").is_in([\n      \"Sri Lanka\", \"Turkey\", \"Poland\", \"South Korea\", \n      \"Poland\", \"South Korea\",\"Malaysia\", \"Russia\", \n      \"Pakistan\", \"Vietnam\", \"Thailand\", \"South Africa\"]))) \n  .select([\"country\", \"infant_mortality\"])\n  .sort(\"infant_mortality\", reverse = True)\n)"
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#aggregates",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#aggregates",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "Aggregates",
    "text": "Aggregates\n\n grouping and taking an average\n\ngapminder %>%\n  group_by(continent) %>%\n  summarise(mean_life_expectancy = mean(life_expectancy) %>%\n              round(2), .groups = \"keep\")\n\n\n\n  \n\n\n\n now with polars\n\n(gapminder\n  .groupby(\"continent\")\n  .agg([\n    (pl.col(\"life_expectancy\")\n        .mean().\n        round(2).\n        alias(\"mean_life_expectancy\"))\n    ])\n  .sort(\"continent\")\n)"
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#with-conditionals",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#with-conditionals",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "With conditionals?",
    "text": "With conditionals?\n\n let‚Äôs do something slightly more complicated\n\ngapminder %<>% \n  mutate(group = case_when(\n    region %in% c(\n      \"Western Europe\", \"Northern Europe\",\"Southern Europe\", \n      \"Northern America\", \n      \"Australia and New Zealand\") ~ \"West\",\n    region %in% c(\n      \"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n    region %in% c(\n      \"Caribbean\", \"Central America\", \n      \"South America\") ~ \"Latin America\",\n    continent == \"Africa\" & \n      region != \"Northern Africa\" ~ \"Sub-Saharan\",\n    TRUE ~ \"Others\"))\n\ngapminder %>% count(group)\n\n\n\n  \n\n\n\n rather than use a case_when style function you can continue to chain .when and .then\n\ngapminder = (gapminder.with_columns(\n  pl.when(\n    pl.col(\"region\").is_in([\n      \"Western Europe\", \"Northern Europe\",\"Southern Europe\", \n      \"Northern America\", \"Australia and New Zealand\"]))\n    .then(\"West\")\n    .when(\n      pl.col(\"region\").is_in([\n        \"Eastern Asia\", \"South-Eastern Asia\"]))\n    .then(\"East Asia\")\n    .when(\n      pl.col(\"region\").is_in([\n        \"Caribbean\", \"Central America\", \n        \"South America\"]))\n    .then(\"Latin America\")\n    .when(\n      (pl.col(\"continent\") == \"Africa\") & \n      (pl.col(\"region\") != \"Northern Africa\"))\n    .then(\"Sub-Saharan\")\n    .otherwise(\"Other\")\n    .alias(\"group\")\n))\n\n(gapminder\n  .groupby(\"group\")\n  .agg([ pl.count() ])\n  .sort(\"group\")\n)\n\n\n\n\n\n  \n\n\n\nI think this is probably a good enough intro to how you‚Äôd generally do things. Filtering, aggregating, and doing case_when style workflows are probably the most foundational and this could already get you started in another language without as much headache"
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#scatterplots",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#scatterplots",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nI‚Äôm trying to strike a balance between dead basic plotly plots and some things you might want to do to make them look a little more the way you want. The great thing about customizing is that you can write functions to do specific things. I don‚Äôt want to overload you with defensive programming for custom function writing using Colin Fay‚Äôs attempt package, so I‚Äôm simplifying a bit; or at least trying to strike a balance. in some instances you can create simple functions or just save a list of values you want to recycle throughout.\n + plotly\n\nplotly_title <- function(title, subtitle, ...) {\n  return(\n    list(\n      text = str_glue(\n        \"\n        <b>{title}</b>\n        <sup>{subtitle}</sup>\n        \"),\n      ...))\n}\n\nmargin <- list(\n  t = 95,\n  r = 40,\n  b = 120,\n  l = 79)\n\ngapminder %>%\n  filter(year == 1962) %>%\n  plot_ly(\n    x = ~fertility, y = ~life_expectancy, \n    color = ~continent, colors = \"Set2\", \n    type = \"scatter\", mode = \"markers\",\n    hoverinfo = \"text\",\n    text = ~str_glue(\n      \"\n      <b>{country}</b><br>\n      Continent: <b>{continent}</b>\n      Fertility: <b>{fertility}</b>\n      Life Expectancy: <b>{life_expectancy}</b>\n      \"),\n    marker = list(\n      size = 7\n    )) %>%\n  layout(\n    margin = margin,\n    title = plotly_title(\n      title = \"Scatterplot\",\n      subtitle = \"Life expectancy by fertility\",\n      x = 0,\n      xref = \"paper\")) %>%\n  config(displayModeBar = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Plotly rendering\n\n\n\nA quick note about having plotly work inside of the RStudio IDE‚Äìas of the time of this writing it isn‚Äôt very straightforward, i.e., not officially supported yet. The plot will open in a browser window and it‚Äôs fairly snappy. The good think is that on the reticulate side, knitting works! So this side was able to put all this together via rmarkdown when I started this post and Quarto now that I‚Äôm finishing this post (remember any  chunk will default to the knitr engine), so that‚Äôs pretty cool. We‚Äôre even using both renv and mamba for both environments in the same file \n\n\n\n + plotly\n\ndef plotly_title(title, subtitle):\n  return(f\"<b>{title}</b><br><sup>{subtitle}</sup>\")\n\nmargin = dict(\n  t = 95,\n  r = 40,\n  b = 120,\n  l = 79)\n  \nconfig = {\"displayModeBar\": False}\n\n(px.scatter(\n  (gapminder.filter(pl.col(\"year\") == 1962).to_pandas()),\n  x = \"fertility\", y = \"life_expectancy\", color = \"continent\",\n  hover_name = \"country\",\n  color_discrete_sequence = px.colors.qualitative.Set2,\n  title = plotly_title(\n    title = \"Scatterplot\", \n    subtitle = \"Life expectancy by fertility\"),\n  opacity = .8, \n  template = \"plotly_white\") \n  .update_traces(\n    marker = dict(\n      size = 7))\n  .update_layout(\n    margin = margin)\n).show(config = config) \n\n\n                        \n                                            \n\n\n\nplotly expects a pandas DataFrame so we‚Äôre just using .to_pandas() to give it what it wants, but that doesn‚Äôt have to stop you from adding any filtering, summarizing, or aggregating before chaining the data into your viz."
  },
  {
    "objectID": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#conclusion",
    "href": "blog/2022-07-r-python-side-by-side/r-python-side-by-side.html#conclusion",
    "title": "A tidyverse R and polars Python side-by-side",
    "section": "Conclusion",
    "text": "Conclusion\n\nHopefully this is helpful. If people like posts like this I can try to do more blogging, I just get busy and foregetful sometimes! Feel free to reach out with any feedback or questions."
  },
  {
    "objectID": "blog/2014-12-python-journey/python-journey.html",
    "href": "blog/2014-12-python-journey/python-journey.html",
    "title": "The Python journey‚ÄìOne Semester with Python",
    "section": "",
    "text": "This was quite a journey for me. I started the same way everyone else has; with my very first ‚ÄúHello World‚Äù program written in Python 3:\n\n\n# Hello World! program.\ndef main():\n    #get the user's name\n    name = input('What is your name? ')\n    print('Hello World! I am', name)\n\n# Call the main function\nmain()\n\nThis was the first assignment for my Programming in Python course. I was not content to have it only print ‚ÄúHello World‚Äù‚Äîno, I need to personalize it in some small way. The following was really interface (if I‚Äôm speaking pythonically); to a wider interest in programming qua programming:\n\n\nname = input('What is your name? ')\n\n\nNevertheless, I was not content with this. I allowed myself to be sucked into a forceful vortex that had me thinking I‚Äôd be using Jupytr notebooks, matplotlib, etc., to show off how much I know about Python from Twitter. Notwithstanding, the above is what was submitted because I didn‚Äôt know how to do any of the fancy stuff I read about. I didn‚Äôt know how to use Pandas. I didn‚Äôt know how to use Blaze. I didn‚Äôt even know how to use ‚Äòconda update conda‚Äô in my terminal (oh; it‚Äôs a package manager‚Äînot just an easy way to install Python 3.4 on my computer at work without Admin privlages!).\n\nThe reality is that I still have a lot to learn‚ÄîI‚Äôm still in the shallow end. Nothing prepared me for the absolute angst associated with trying to implement a (beginner‚Äôs attempt at) the Object Oriented Programming (OOP) paradigm as a final extra-credit assignment! I didn‚Äôt even know I had been writing, although very functional; or, very function reliant, procedural code. Somewhere between nesting lists inside of dictionaries, iterating over them, and implementing ‚Äòtry, except‚Äô statements, I thought I was really going places with my code. OOP razed that sandcastle quite briskly. Like a kind of soverign and violent natural phenomena.\n\nFrom my first program to my 10th program, this is how far I have come. This is my attempt at OOP, classes, ‚Äòinit‚Äô methods, inheritance composition, and more. It‚Äôs likely pretty flawed and could be made less redundant, but I didn‚Äôt copy StackOverflow and tried to figure it out on my own; so, I‚Äôm damn proud of it! There were some programs specs that I needed to show an understanding of; quickly, the program specs:\n\n\nEach question will have four possible answers\nEach player will take turns with the questions\nThere will be a total of 10 questions, each player getting a chance to answer five of them. If the player selects the correct answer, they earn a point. Tell the player whether they got it right or wrong.\nMust create a ‚ÄòQuestion‚Äô class to hold data with the following attributes:\n\nA trivia question\nPossible answer 1\nPossible answer 2\nPossible answer 3\nPossible answer 4\nThe number of the correct answer, e.g., 1, 2, 3, or 4\n\nQuestion class must have an ‚Äòinit‚Äô method, accessors, mutators, and a ‚Äòstr‚Äô method.\nUse value-returning functions; one named createQuestionsAnswers() that creates the list to display questions and keeps tracks of user input to let players know if they won, lost, or tied.\n\n\nHere are my solutions:\n\n\n# -*- coding: utf-8 -*-\n\"\"\"     A10--Trivia Game!\n        --> two player trivia game\n        --> OOP approach to building the game with classes and objects\n\"\"\"\nimport csv\nimport random\n\n# the Question class acts as a placeholder for the parts of the question\n# needed to construct questions and check answers\nclass Question:\n    # __init__ uses the Data class method getData through composition\n    def __init__(self, question, a1, a2, a3, a4, answer, ansNum):\n        self.getData = Data('csv')\n        self.question = question\n        self.a1 = a1\n        self.a2 = a2\n        self.a3 = a3\n        self.a4 = a4\n        self.answer = answer\n        self.ansNum = ansNum\n\n    # the method performs better as a class method since it instantiates the\n    # Question class with sample questions for the game\n    @classmethod\n    def getQuestion(cls, triviaDict):\n        # using random to get 10 random numbers between a specific range for\n        # trivia questions\n        randomGenerator = random.sample(range(1, 817), 1)\n\n        # for an individual random number in the sample range\n        # --> iterate and use number as index for the trivia questions\n        for i in randomGenerator:\n            question = triviaDict[i][0]\n            a1 = triviaDict[i][1]\n            a2= triviaDict[i][2]\n            a3 = triviaDict[i][3]\n            a4 = triviaDict[i][4]\n            answer = triviaDict[i][5]\n            ansNum = triviaDict[i][6]\n\n        # this creates an instance to return (from question class)\n        aQuestion = Question(question, a1, a2, a3, a4, answer, ansNum)\n        return aQuestion\n\n    # this is a part of using composition rather than inheritance to get the\n    # attributes from the getData method\n    def __getattr__(self, attr):\n        return getattr(self.getData, attr)\n\n    # this method sets up the question (also checks answer)\n    @classmethod\n    def setupAsk(cls, q):\n        print('\\n', q.question, '\\n\\t1: ', q.a1, ' \\\n                 \\n\\t2: ', q.a2, '\\n\\t3: ', q.a3, ' \\\n                 \\n\\t4: ', q.a4, '\\n')\n\n        # make sure the user's input works\n        while True:\n            try:\n                choice = int(input(\"\\nWhat's your answer? \\n--> \"))\n            except ValueError:\n                print('\\nSorry, the answer only accepts numbers; please \\\n                       enter a number 1-4')\n                choice = int(input(\"\\nWhat's your answer? \\n--> \"))\n            finally:\n                if choice in range(1, 5):\n                    break\n\n        # if the question is correct, return true; if not, return false\n        if choice == q.ansNum:\n            print('\\nCorrect! \\n', q.answer)\n            return True\n        elif choice != q.ansNum:\n            print('\\nIncorrect!  \\n', q.answer)\n            return False\n\n# the Data class handles openning the file and preparing it to be used by\n# the Question and Game class\nclass Data:\n    def __init__(self, filetype):\n        self.filetype = filetype\n\n    # opens the CSV to read and prepare it to be used in computaiton later\n    @classmethod\n    def getData(cls):\n        # make sure there isn't an IO error\n        try:\n            # open the csv file + use an index accumulator for dictionary\n            with open('trivia.csv') as csvFile:\n                readCSV = csv.reader(csvFile, delimiter=',')\n                index = 0\n\n                # questions, answer choices, and answers dictionary\n                rowDict = {}\n                questionData = {}\n                # reading the trivia questions\n                for row in readCSV:\n                    rowDict[index] = row\n                    question = rowDict[index][0]\n                    a1 = rowDict[index][1]\n                    a2 = rowDict[index][2]\n                    a3 = rowDict[index][3]\n                    a4 = rowDict[index][4]\n                    answer = rowDict[index][5]\n\n                    # figure out which answer is correct and assign a variable\n                    if answer == a1:\n                        ansNum = 1\n                    elif answer == a2:\n                        ansNum = 2\n                    elif answer == a3:\n                        ansNum = 3\n                    elif answer == a4:\n                        ansNum = 4\n                    else:\n                        print(\"Error!  No correct answer\")\n\n                    # place questions into new dictionary in the right order\n                    questionData[index] = [question, a1, a2, a3, a4, \\\n                                            answer, ansNum]\n                    index += 1\n\n                return questionData\n\n        except IOError:\n            print(\"The file could not be found.\")\n\n# the Game class is where the bulk of the game's structure is found\nclass Game:\n    def __init__(self, playerID, gamePoints):\n        self.playerID = playerID\n        self.gamePoints = gamePoints\n\n    # method to create instances for questions and find out if a quesiton\n    # was answered correctly or not\n    def round(self, qClass, data):\n        gamePoints = 0 # reset to 0 for new round\n\n        # instances\n        q1 = qClass.getQuestion(data)\n        q2 = qClass.getQuestion(data)\n        q3 = qClass.getQuestion(data)\n        q4 = qClass.getQuestion(data)\n        q5 = qClass.getQuestion(data)\n\n        # return value is true or false; this computes points\n        if qClass.setupAsk(q1) == True:\n            gamePoints += 1\n        if qClass.setupAsk(q2) == True:\n            gamePoints += 1\n        if qClass.setupAsk(q3) == True:\n            gamePoints += 1\n        if qClass.setupAsk(q4) == True:\n            gamePoints += 1\n        if qClass.setupAsk(q5) == True:\n            gamePoints += 1\n\n        # let the user know what happenned this round\n        print('you won {} points this game!'.format(gamePoints))\n\n        return gamePoints\n\ndef main():\n    # local variables\n    flag = False\n    gameNum = 1\n\n    # instance of Data class\n    data = Data('csv')\n    questionsData = data.getData()\n\n    # instance of Question class with filler data\n    questions = Question('question', 'a1', 'a2', 'a3', 'a4', \\\n                         'answer', 'ansNum')\n\n    # create both players\n    playerOne = Game(str(input('PLAYER ONE//\\nEnter your name: ')), 0,)\n    playerTwo = Game(str(input('PLAYER TWO//\\nEnter your name: ')), 0,)\n\n    # while loop to keep the game going if the user chooses\n    while flag != True:\n        # let the user know which round they're playing\n        print('\\nROUND ', gameNum, '//\\nPlayer One')\n\n        # first player instance; asks five questions\n        p1round = playerOne.round(questions, questionsData)\n\n        print(\"\"\"\n        +++++++++++++++++++++++++++++++++++++++++++++++++\n        +                   SWITCH PLAYERS!             +\n        +++++++++++++++++++++++++++++++++++++++++++++++++\n        \"\"\")\n\n        # let the user know to switch players\n        print('\\nROUND ', gameNum, '//\\nPlayer Two')\n\n        # second player instance; asks the five quesitons\n        p2round = playerTwo.round(questions, questionsData)\n\n        # let the user know which round they are on with accumulator\n        gameNum += 1\n\n        # figure out who won and use user's inputed name and their points\n        # in print statement\n        if p1round < p2round:\n            print('Thank you for playing!  {} is the winner with {} total \\\n                  game points!'.format(playerTwo.playerID, p2round))\n        elif p2round < p1round:\n            print('Thank you for playing!  {} is the winner with {} total \\\n                  game points!'.format(playerOne.playerID, p1round))\n        elif p2round == p1round:\n            print('There was a tie!  Both {} and {} both earned {} total \\\n                   game points; but you are both still \\\n                   winners!'.format(playerOne.playerID, playerTwo.playerID, \\\n                   p1round))\n\n        # find out if user wants to continue + validate user response\n        while True:\n            try:\n                choice = str(input(\"\\nKeep playing? \\n--> \")).upper()\n            except ValueError:\n                print(\"Sorry, enter either a 'Y' for 'Yes', or 'N' for 'No'.\")\n            finally:\n                if choice == 'N':\n                    flag = True\n                    break\n                if choice == 'Y':\n                    break\n                else:\n                    print(\"please enter either a 'Y' for 'Yes', or 'N' \\\n                           for 'No'.\")\n\n    # say bye to players and quit the program\n    print('\\nThank you for playing!  See you next time!\\n')\n\nmain()\n\n\nI couldn‚Äôt help but think about Plato when I was trying to understand how Objects work in Python. There‚Äôs something really similar about how a ‚Äòclass‚Äô has a kind of ontos‚Äîthat it isn‚Äôt just a blueprint‚Äîit exists, and it did exist before the ‚Äòinit‚Äô method gave it attributes; that prescriptive human speculation we come up with when describing the form of something abstract like ‚Äòthe Beautiful‚Äô (although it‚Äôs been a while since my Phil 100A course at UCLA‚ÄîI hope I‚Äôm not misrepresenting the Phaedo).\n\nThe course is over but I have a few titles I purchased from Packt to dig a little deeper. Suggestions are always welcome; the journey‚Äôs telos is to learn; and learn I intend to do!"
  },
  {
    "objectID": "blog/2015-08-p-hacking-and-ontology/p-hacking-and-ontology.html",
    "href": "blog/2015-08-p-hacking-and-ontology/p-hacking-and-ontology.html",
    "title": "P-hacking and ontology",
    "section": "",
    "text": "In a recent FiveTirtyEight post by Christie Aschwanden about researcher bias and P-Hacking, there is a lovely interactive example of what variables a researcher would need include/exclude in the analysis in order to obtain a result that is statistically significant, i.e., p‚â§.05; thus worthy of publishing. The article brought many thoughts to mind, which I am using this blog post to note. The crux of the article, I feel, rests in the following passage from the article:\n\n\nThe important lesson here is that a single analysis is not sufficient to find a definitive answer. Every result is a temporary truth, one that‚Äôs subject to change when someone else comes along to build, test and analyze anew.1\n\n\nThis annotates the concept of truth; introducing time and the transitory nature of what we know as ‚Äòtruth‚Äô. However, if we can think about what it means for some subject to be at one point in time and to be at some other point in time, this, again, introduces the concept of ontology; i.e., what does it mean for a subject to exist? There is a very novel explanation from Joseph Tennis that describes how both time and ontology interact that I would like to introduce:\n\nKnowledge changes through time. Classification schemes as tools for accessing knowledge undergo constant revision. It is impossible to claim that the ontology of subjects and their interrelationships, once established by a classificationist, remain constant within that scheme. As revisions to classification schemes emerge, so too do new subjects. These, new parts of the updated classification scheme are elements in a formal system‚Äîelements that represent the current interpretation of knowledge.2\n\n\nThis is both simple and profound. Time is an interesting concept and it has, for the most part, been missing from the philosophical discourse from Heraclitus to Heidegger.3 If we can remember from the Heraclitus Fragments, ‚ÄúYou cannot step twice into the same stream. For as you are stepping in, other waters are ever flowing on to you‚Äù.4 This statement is perhaps the most profound philosophical aphorism I have encountered since I began to love learning. It decimates our ability to truly understand things metaphysically.5 Nevertheless, it lets you peek, at least in part, at something beyond our anthropocentric naive empiricism. This is important, because there is rich meaning in the way subjects change over time‚Äîit can tell us a lot about ourselves. There is another passage I like in Tennis‚Äô paper that I‚Äôd like to return to:\n\n\n What kind of access is granted by a classification system that shows how knowledge has changed, verses one that revises classes, denying access to the classificationist‚Äôs interpretation of the change in knowledge? With each revision, a scheme for classification cuts itself off from its previous view of knowledge, building an artificial boundary of time. There are other rhetorical questions pertinent to time as it relates to subject access. For example, could one access the array of subjects in higher education that were taught during Plato‚Äôs Greece? Through a classification scheme, can one collocate the works of proto-anthropologists? These knowledges are not reflected in classification schemes, because each living scheme needs to be revised to be viable‚Äîthereby eliminating the fossil record of literary warrant. To what degree do revised classification schemes blind us to how subjects change and are re-collocated through time? What can knowledge organization thery do to help the sophisticated user re-collocate knowledge through time? This can be answered by charting the development of a class in a classification system through time. In other words, this can be answered by charting the subject‚Äôs ontogeny.6\n\n\nThis really reminds me a lot of Heidegger‚Äôs On the Origin of the Work of Art and both his concept of ‚Äúworld‚Äù and the ‚Äúwork of art‚Äù, which has its own poetic interaction. A former professor of mine at UCLA named John McCumber helped shape the way I think philosophically. He was one of the most intimidating professors I had; able to recall from memory huge passages of Hegel, Heidegger, and Nietzsche in German, fluent in French, and writing ancient Greek and Latin on the whiteboard when explaining concepts. In his book Poetic Interaction: Language, Freedom, Reason, there is a passage I think ties some of these floating ideas together:\n\n\n[P]reserving the work of art functions similarly to Being-toward death in Being and Time. Heidegger in fact goes on to relate the preservation of the work of art to the concept of ‚Äúresoluteness‚Äù presented in the book. But the difference is unmistakeable. The inarticulateness of resolve in Being and Time is replaced with the concrete individuality of the work of art, which speaks to us, not from within our world or as an indeterminate ‚Äúcall‚Äù out of it, but from another concrete world, one unique to itself. This way of experiencing a work of art is a condition for its being a work of art at all. An art work which does not deserve an audience, we must say, is no art work. This is why, in experiencing a work of art for what it is, we ‚Äúpreserve‚Äù it; and it is why, for Heidegger as for Hegel, the work of art is intrinsically a communal and (in a broad sense) a communicative entity.7\n\n\nThis may be confusing at first, because the quote is taken a bit out of context, but the phrase that captivates me is the section where McCumber talks of the work of art being preserved within its own world. Ontologically this is fascinating because the work of art has aged‚Äîit is not as it was when created. Moreover, it is not necessarily a part of this world either; it is preserved in some capacity. My main interest in subject ontogeny is the attemp to make explicit the effect time has on our body of knowledge. Being able to understand what encyclopedic resources were available when Plato was alive is a form of artistic expression because it crafts an experience that people can interact with‚Äîit would preserve Plato‚Äôs world in a small sense and that world would rest apart from our own world. That we could be transported is the exciting notion; that we could stretch our conceptions, biases, cultural stereotypes, all the mental baggage we carry around in our mind from being thrown into the world at this particular time in its existence‚Äîthis would provide perspective that moves our species forward.\n\nSo, what does this have to do with P-Hacking? It is that truth is temporary. Results, data, these things are not stateless; they say as much about this time that we are doing research as they do informing us of new temporary truths. We could argue about what is currently axiomatic and how certain certainties are foundational, but this is only an affirmation of a kind of thinking that needs the external world to conform to our senses and instruments to make meaning. As humans we are always making meaning and mistakes‚Äîthose things go hand in hand. Like biological life, knowledge is iterative and it evolves. As our knowledge evolves, so too should our way of describing that evolution. The philosophical notion of ontology can be therefore connection to the information science concept of ontology8 if we are able to represent human knowledge‚Äôs interrelationships that describe what we know, temporally, to be true for all domains of discourse in as many ways as can be accurately and approximately completed. Such an ontology would reflect in an abstract and concrete way what it is to be human. What would tell us who we are.\n\nScience is a bit of a dialectic‚Äîthere is movement in going from an unknown to a known that has a telos that can and should be represented in an ontology. Therefore, to say that we know something is to make an epistemological statement, which begs an epistemological question: how do you know that? When our understanding of science is as Aschwanden says:\n\nScience is not a magic wand that turns everything it touches to truth. Instead, ‚Äúscience operates as a procedure of uncertainty reduction [‚Ä¶] [t]he goal is to get less wrong over time.‚Äù This concept is fundamental‚Äîwhatever we know now is only our best approximation of the truth. We can never presume to know everything.9\n\nThat this line of thinking is novel for people in STEM fields is as disconcerting as the lack of statistical knowledge in the humanities. I see this as perhaps the most pervasive mistake in higher education. On the one hand, we are told not to be a generalist; to specialize, to find a niche and exploit it. On the other hand, we miss out on perspectives, ways of thinking, and challenges that provoke deeper thinking than would normally be the case. In working toward teaching myself computer science and mathematics, I am continually pushing myself to broaden my understanding of the world. How are STEM only researchers doing the same? Without a complete education that covers all domains at least in part we lack the tools to truly understand the 21st century. We need interdisciplinary teams of interdisciplinarians; not callow one-faceted domain caricatures that cannot think in multiple loci. I believe this will enable humanity to probe deeper into complexity and how ecologically interdependent we all are on Earth.\n  \n\n\n\n\nFootnotes\n\n\nAschwanden, C. (2015, August 19). Science Isn‚Äôt Broken: It‚Äôs just a hell of a lot harder than we give it credit for. Retrieved August 25, 2015, from https://fivethirtyeight.com/features/science-isnt-broken/‚Ü©Ô∏é\nTennis, J. T. (2002). ‚ÄúSubject Ontogeny: Subject Access through Time and the Dimensionality of Classification.‚Äù In Challenges in Knowledge Representation and Organization for the 21st Century: Integration of Knowledge across Boundaries: Proceedings of the Seventh International ISKO Confrence. (Granada, Spain, July 10-13, 2002). Advances in Knowledge Organization, vol. 8. Wurzburg: Ergon: 54-59.‚Ü©Ô∏é\nI inherited this opinion from Dr.¬†John McCumber through his various philosophy courses and seminars. See http://www.germanic.ucla.edu/people/faculty/mccumber/ for more info.‚Ü©Ô∏é\nhttps://en.wikiquote.org/wiki/Heraclitus‚Ü©Ô∏é\nWhat I mean is that, when reading the passage, we are in the river; the thing we are trying to classify‚Äîto understand. The fact that we have no bird‚Äôs eye view is an important caveat to working toward understanding the world in a realistic way. We call it one thing, but it is something else outside of the way humans communicate it to each other through language.‚Ü©Ô∏é\nTennis (2002)‚Ü©Ô∏é\nMcCumber, J. (1989). Poetic interaction: Language, freedom, reason. Chicaco: University of Chicago Press‚Ü©Ô∏é\nhttps://en.wikipedia.org/wiki/Ontology_(information_science)‚Ü©Ô∏é\nAschwanden (2015)‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/2018-03-uptake-fellowship-start/uptake-fellowship-start.html",
    "href": "blog/2018-03-uptake-fellowship-start/uptake-fellowship-start.html",
    "title": "Reflections on the start of my data fellowship with Uptake",
    "section": "",
    "text": "I can‚Äôt believe it‚Äôs been more than a week since I returned from my trip to Chicago. This was an incredible opportunity and before I start this post a few thank yous are in order: I really can‚Äôt thank Uptake enough for being the kind of company that does more than just ‚Äòhave an mission statement‚Äô‚Äìhaving the motivation to start a philanthropic arm (the .org side of Uptake) shows the company is serious about the work they intend to do in the sector. This is evident by the data fellows program specifically and I would be remiss if I did not personally thank Andrew Means for designing such a wonderful program in the data for good space. That folks like me miss out on mentoring and growth because we‚Äôre often the only data folks in our organization hit very close to home. So, feeling very grateful all around üòÑ\n\n\n\nPre(R)AMBLE\n\nI always feel that it is important to highlight the path I took to get to the point where I am‚Äìthe non-traditional route, which begs the question, ‚Äúwhat is the traditional route to data science?‚Äù Rather than go down a semantics rabbit hole I‚Äôd rather point out some of the raw ingredients that, at least I believed when I started down this path, make it easier to get into data science/analysis:\n\n\n\n A good working knowledge of computer science (concepts and practical experience)\n\n\n All the foundational mathematics courses: Algebra to Calculus\n\n\n Some upper division statistics coursework in undergrad and some applied research setting or exposure to experimental design\n\n\n\nWhen I decided that I really wanted to work in this sector this is how I felt:\n\n\n\n‚ùå I once built an HTML page in the 10th grade about Chrono Trigger locally on a computer I built after the debut of the Pentium III chip so I could finally afford a the Pentium II chip AMA\n\n\n‚ùå Oh god, mathematics was my least favorite subject in school (tied with orthography)\n\n\n‚ùå I took intro to stats for my general education requirements but did not really pay attention üò≠\n\n\n\nWhen you feel insecure about what you assume other people know it creates a lot of anxiety not only about what you should learn, but also how you should go about learning it and to what level you need learn it in order to have this ‚Äòworking knowledge‚Äô of said thing. I‚Äôve been battling these thoughts for almost three years now and even before I flew to Chicago I kept asking myself: am I even ready for an opportunity like this?\n\nHere‚Äôs my answer; not just for myself but for anyone else reading this who knows what this feels like: YES üéâ\n\nYES because there is just too much out there for one person to learn and/or know. Jumping into something unfamiliar is sort of like getting married or becoming a parent: you are never ready; the process of becoming is what readies you. So keep it up!\n\n\n\nMy main take aways\n\n\n\n I am actually making progress!\n\n\n My work output as product\n\n\n A little asceticism goes a long way\n\n\n\n\n\nI am actually making progress!\n\nThis was the most encouraging part of the experience‚Äìthat the past few years of hard work googling things I read on twitter and pouring over blog posts, making my way through Stats and ML books has been working. It‚Äôs encouraging to finally feel like you‚Äôre at a point where you can really benefit from a mentor, which is a terrific feeling. So much about ML used to go over my head when I jumped into data work with this loony desire to someday become a ‚Äòdata scientist and beyond‚Äô that it‚Äôs difficult to calibrate when I‚Äôm able to say, ‚Äòyes‚Äô I can finally enter the inner chamber of ‚Äòdata science‚Äô and really quit the preparation sojourn and begin the rewarding journey ahead. I finally feel like the moment has arrived.\n\nTalking with my other fellows and a handful of the data scientists at Uptake, working through workshop materials, having long conversations about different ML frameworks both within R and outside of the R ecosystem over dinner and on Slack without feeling like I‚Äôm totally lost has fueled the üî• to keep working harder and to not lose sight of the goal I still have. I am and always plan to be a life-long learner and this is good.\n\n\n\nMy work output as a product\n\nThis was an epiphany. The reports and dashboards and answers to questions that come my way at work is a product. And wearing a product manager hat as a data scientist is a really good skill for me to have‚Äìespecially when I‚Äôm often bridging three domains by myself: engineering, product management, data science. Making sure I‚Äôve really scoped what I intend to do before I start pulling data, munging, modeling, or communicating is something I just wasn‚Äôt doing.\n\nThe second part of the epiphany was that there really isn‚Äôt anything wrong with a Minimum Viable Product (MVP) because it is the foundation for future iterations. After having the back and forth during a scoping process about what is possible/feasible, making your product effective is not measured in how tight and clean your code is or how complicated the model is; it‚Äôs in how well you‚Äôve managed to satisfy the party that needs your expertise. The easiest way to see if you‚Äôre on the right track is to provide a MVP early to see how well it does in satisfying what was scoped out. If it doesn‚Äôt work and more scoping is required; cool, you didn‚Äôt waste any time polishing an artifact destined to the archive heap. If it works then there is ample room/time to really polish the product the way you might like.\n\nThis is my personal downfall as I care wayyy too much about how the things I make look üëÄ that I spend wayyy too much time going in a direction that might ultimately be inconsequential to the product‚Äôs audience. This is something I am really trying to internalize more.\n\n\n\nA little asceticism goes a long way\n\nMy data science mentor Andrew Hillard said something to me that really hadn‚Äôt occurred to me when I was explaining that I wanted to learn to build a Bayesian multi-level model for my project‚ÄìI wanted to do this in Stan and really get a thorough grounding in this kind of approach. He said (I‚Äôm paraphrasing), ‚Äúone of the things I‚Äôve had to learn here is that this isn‚Äôt a Kaggle competition‚Äìthat often interpretable models like logistic regression perform very well in a surprising amount of use cases. Getting a model to perform 90% of the time is already a success in a lot of ways because going beyond that will take more time and may not operationally improve what the model is predicting is a dramatic way‚Äù. This is probably really difficult for those of us who really get excited about using something we just learned to tackle a problem, which I am in no way saying ‚Äòdon‚Äôt do that‚Äô; what I‚Äôm saying is that often I have very limited time and in order to try and move the organization into a data driven future, there are many opportunities to deny myself the joy of over-complicating things when it isn‚Äôt necessary. This is something I am going to try and keep in mind as I work on my project for the data fellows program and as this process begins to unfold.\n\n\n\nSo what is my project + what are my goals for my program?\n\nI will be writing a separate post about this but here is the high-level highlight:\n\nTo develop deeper machine learning intuition around the kinds of models that maximize interpretability/simplicity with performance in order to drive the reduction in preventable homelessness recidivism by predicting negative housing exits early and to target more supportive services to those residents who are in need. By shifting the focus from ‚ÄòHousing First‚Äô to ‚ÄòHousing Always‚Äô, I‚Äôm hoping that I can build good scaffolding through this data for social work professionals to use in developping new ways to diliver programming."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Professionally\nMy background is in theology, philosophy, and continental thought. This formed the basis for my academic exploration. My BA was in both French and comparative literature at UCLA. In the US, the ‚Äòcomp lit‚Äô field is dominated by literary criticism and the search for universal truths and archetypes in global literature, which is not always apparent from the field‚Äôs name. Additionally, I took a fair amount of philosophy courses (my major originally), German language courses, and a bit of Scandinavian literature, film, and Old Norse (inspired from a trip to Iceland).\n\nMy Master‚Äôs degree is in Information Science (MSIS). Specifically, Information Systems. The MSIS is designed for information work in many different fields and is a bit of an odd degree for someone with my work experience. The information systems track taught me how to support my organization by thinking systematically and procedurally; and I combine that with my background in critical theory to ask why some formalization exists‚Äìand after all the facts are gathered‚Äìwhether or not there is a better way to operationalize something.\n\nProfessionally I‚Äôve gone from being the sole data person doing, what I suppose we could call full stack data science to leading a data science team within local government. Given the limited resources of nonprofits and local gov, I have a lot of experience finding a way to get things off the ground without a ton of resources. The longer I work in roles like this the more I end up drifting more and more into DevOps territory. As a leader, I want to figure out the best way for my team to work and often that means figuring out the infrastructure so they don‚Äôt have to. I do my best to create an environment that allows them to do as much dev work with the least amount of distractions as possible.\n\n\n\nPersonally\nI‚Äôve been lucky enough to do a fair amount of traveling; a few months in Japan at 19, a year in Paris at 23/24, Summer in Germany and Scandinavia at 24, and some quick trips to different places while spending quality time in these locations. These experiences have shaped who I am as an individual, before I worked up the courage to talk to a girl sitting across from me on the Metro. This changed everything. I went from an individual to a couple to a family in what felt like no time at all.\n\nI hope to contribute posts/projects at the intersection of my various interests. I would like to have more time to explore the semantic relationships between objects and concepts in time. Also, I can never seem to escape the epistemological underpinnings of things and often spend time thinking about how probability and statistics is grounded. Deep down I am fairly optimistic: I believe that everything is connected and that the 21st century will be about discovering these connections‚Äî-hopefully I can be a part of this. At least in some small way."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Romuald ZAMI",
    "section": "",
    "text": "A tidyverse R and polars Python side-by-side\n\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2022\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nReflections on the start of my data fellowship with Uptake\n\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2018\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nAdvent of code in R: day one\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2017\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nBuilding this site with RStudio and rmarkdown\n\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2016\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nThe move to R\n\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2016\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nRecreating the vaccine heatmap in plotly 4.0 with R\n\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2015\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nP-hacking and ontology\n\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2015\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nBar chart annotations with pandas and matplotlib\n\n\n\n\n\n\n\n\n\n\n\n\nJun 15, 2015\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nMy pandas snippets‚Äìalways evolving\n\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2015\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nFirst Kaggle Submission‚ÄìRandom Forest Classifier\n\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2015\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nThe Python journey‚ÄìOne Semester with Python\n\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2014\n\n\nRobert Mitchell\n\n\n\n\n\n\n\n\nFoucault‚Äôs challenge to modernist classification\n\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2013\n\n\nRobert Mitchell\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#fa-hand-peace-vous-trouverez-sur-ce-blog-un-ensemble-de-documents-relatifs-aux-language-fa-rstudio-fa-champagne-glasses",
    "href": "index.html#fa-hand-peace-vous-trouverez-sur-ce-blog-un-ensemble-de-documents-relatifs-aux-language-fa-rstudio-fa-champagne-glasses",
    "title": "",
    "section": " Vous trouverez sur ce blog un ensemble de documents relatifs aux language  ",
    "text": "Vous trouverez sur ce blog un ensemble de documents relatifs aux language"
  },
  {
    "objectID": "index.html#fa-hand-peace-vous-trouverez-sur-ce-blog-un-ensemble-de-documents-relatifs-aux-language-fa-r-project-et-plus-encore-fa-puzzle-piece",
    "href": "index.html#fa-hand-peace-vous-trouverez-sur-ce-blog-un-ensemble-de-documents-relatifs-aux-language-fa-r-project-et-plus-encore-fa-puzzle-piece",
    "title": "robertmitchellv",
    "section": " Vous trouverez sur ce blog un ensemble de documents relatifs aux language  et plus encore‚Ä¶",
    "text": "Vous trouverez sur ce blog un ensemble de documents relatifs aux language  et plus encore‚Ä¶"
  },
  {
    "objectID": "index.html#fa-hand-peace-vous-trouverez-sur-ce-blog-un-ensemble-de-documents-relatifs-aux-language-fa-circle-r-et-plus-encore-fa-puzzle-piece",
    "href": "index.html#fa-hand-peace-vous-trouverez-sur-ce-blog-un-ensemble-de-documents-relatifs-aux-language-fa-circle-r-et-plus-encore-fa-puzzle-piece",
    "title": "robertmitchellv",
    "section": " Vous trouverez sur ce blog un ensemble de documents relatifs aux language  et plus encore‚Ä¶",
    "text": "Vous trouverez sur ce blog un ensemble de documents relatifs aux language  et plus encore‚Ä¶"
  },
  {
    "objectID": "index.html#fa-hand-peace-vous-trouverez-sur-ce-blog-un-ensemble-de-documents-relatifs-aux-language-fa-brands-r-project-et-plus-encore-fa-puzzle-piece",
    "href": "index.html#fa-hand-peace-vous-trouverez-sur-ce-blog-un-ensemble-de-documents-relatifs-aux-language-fa-brands-r-project-et-plus-encore-fa-puzzle-piece",
    "title": "Romuald ZAMI",
    "section": " Vous trouverez sur ce blog un ensemble de documents relatifs aux language  et plus encore‚Ä¶",
    "text": "Vous trouverez sur ce blog un ensemble de documents relatifs aux language  et plus encore‚Ä¶"
  },
  {
    "objectID": "index.html#vous-trouverez-sur-ce-blog-un-ensemble-de-documents-relatifs-aux-language-fa-brands-r-project-et-plus-encore-fa-puzzle-piece",
    "href": "index.html#vous-trouverez-sur-ce-blog-un-ensemble-de-documents-relatifs-aux-language-fa-brands-r-project-et-plus-encore-fa-puzzle-piece",
    "title": "Romuald ZAMI",
    "section": "Vous trouverez sur ce blog un ensemble de documents relatifs aux language  et plus encore‚Ä¶",
    "text": "Vous trouverez sur ce blog un ensemble de documents relatifs aux language  et plus encore‚Ä¶"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Romuald ZAMI",
    "section": "",
    "text": "Vous trouverez sur ce blog un ensemble de documents relatifs au language  et plus encore‚Ä¶\n Ci dessous, les documents les plus r√©cents :\n\n\n\n\n\nBlog\n\n\n\n\n\nA tidyverse R and polars Python side-by-side\n\n Jul 19, 2022\n\nNo matching items\n\n\n\n\n\n\nR√©alisations\n\n\n\n\n\nCours de visualisation des donn√©es\n\n Oct 31, 2022\n\nNo matching items\n\n\n\n\n\n\nProjets\n\n\n\n\n\nLA County COVID-19 Dashboard\n\n Apr 2, 2020\n\nNo matching items"
  },
  {
    "objectID": "talks/Cours manipulation/index.html",
    "href": "talks/Cours manipulation/index.html",
    "title": "Cours de manipulation des donn√©es",
    "section": "",
    "text": "Servane.Gey@parisdescartes.fr - Bureau B4-18 \n\nDocuments : Moodle, COMMUN\nR√©f√©rences (liste non-exhaustive) :\n\nCours de F.-X. Jollois : https://fxjollois.github.io/cours-2018-2019/stid-2afa‚Äìprog-r/\nR for Data Science, H. Wickham & G. Grolemund, O‚ÄôReilly, (https://r4ds.had.co.nz/)\nThe Grammar of Graphics, L. Wilkinson, Springer (https://www.springer.com/us/book/9780387245447)\nTidyverse : https://tidyverse.tidyverse.org/\nVisualisation : https://ggplot2.tidyverse.org/reference/\nApplications Web : https://shiny.rstudio.com/\nR Markdown : https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf\nRStudio Cheat Sheets : https://www.rstudio.com/resources/cheatsheets/"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#organisation-et-validation",
    "href": "talks/Cours manipulation/index.html#organisation-et-validation",
    "title": "Cours de manipulation des donn√©es",
    "section": "Organisation et validation",
    "text": "Organisation et validation\n\nOrganisation :\n\n33h de cours et TP r√©partis sur 11 s√©ances de 3h\n\nValidation :\n\n1 TP not√© en milieu de semestre (50 %)\n1 DST sur machine en fin de semestre (50 %)"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#plan-du-cours",
    "href": "talks/Cours manipulation/index.html#plan-du-cours",
    "title": "Cours de manipulation des donn√©es",
    "section": "Plan du cours",
    "text": "Plan du cours\n\nManipulation et analyse de donn√©es\n\nImportation et exportation de donn√©es (package readr)\nManipulation de tables de donn√©es (package dplyr)\nStatistiques : quelques exemples avec dplyr\n\nVisualisation de Donn√©es\n\nRepr√©sentations graphiques (package ggplot2)\nCartographie (package leaflet)\n\nFonctions\n\nEcriture de fonctions avec dplyr\nFonctions sp√©cifiques √† l‚Äôanalyse de donn√©es\nGestion des sorties sous forme de listes\n\nRapports et Pr√©sentations\n\nRappels sur R Markdown\nR√©daction de rapports et de pr√©sentations\n\nApplications Web\n\nRappels sur R Shiny\nTableaux de bord (package shinydashboard)\nConception d‚Äôune application avec interface utilisateur"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#installation-de-r-et-rstudio-en-local",
    "href": "talks/Cours manipulation/index.html#installation-de-r-et-rstudio-en-local",
    "title": "Cours de manipulation des donn√©es",
    "section": "Installation de R et RStudio en local",
    "text": "Installation de R et RStudio en local\n\nT√©l√©charger la derni√®re version de R adapt√©e √† votre syst√®me d‚Äôexploitation (Windows, Mac OS, Linux) via le CRAN de l‚Äôuniversit√© de Lyon : https://pbil.univ-lyon1.fr/CRAN/\nT√©l√©charger la derni√®re version de RStudio adapt√©e √† votre syst√®me d‚Äôexploitation (Windows, Mac OS, Linux) via le site rstudio : https://rstudio.com/products/rstudio/download/\nInstaller les 2 logiciels en √©x√©cutant les fichiers t√©l√©charg√©s.\nAttention RStudio est une interface graphique tr√®s √©volu√©e pour une utilisation simplifi√©e du logiciel R. L‚Äôinstallation de R au pr√©alable est donc indispensable avant toute utilisation de RStudio.\nL‚Äôinstallation des librairies (ou packages) peut se faire directement via la console, ou via l‚Äôinterface de RStudio."
  },
  {
    "objectID": "talks/Cours manipulation/index.html#serveur-rstudio",
    "href": "talks/Cours manipulation/index.html#serveur-rstudio",
    "title": "Cours de manipulation des donn√©es",
    "section": "Serveur RStudio",
    "text": "Serveur RStudio\n\nServeur d√©di√© √† l‚Äôutiliation de R, dont la derni√®re version y est install√©e, avec toutes les librairies r√©guli√®rement mises √† jour.\nConnexion sur l‚Äôinterface RStudio directement via la connexion sur le serveur.\nServeur RStudio de l‚ÄôIUT (disponible uniquement en local, i.e.¬†sur les ordinateurs de l‚ÄôIUT) : hhtp://rstudio.iutparis.local:8787/\nPremi√®re connexion : login et mot de passe = login de l‚ÄôIUT.\nChanger imm√©diatement votre mot de passe (voir le mail de M. Jollois)."
  },
  {
    "objectID": "talks/Cours manipulation/index.html#introduction",
    "href": "talks/Cours manipulation/index.html#introduction",
    "title": "Cours de manipulation des donn√©es",
    "section": "Introduction",
    "text": "Introduction\n\nR :\n\nlangage de programmation pour le traitement des donn√©es\nfonctions propres int√©gr√©es au langage\nnombreuses librairies\n\ntidyverse :\n\nensemble de librairies d√©velopp√©es par l‚Äô√©quipe de R Studio\nlibrairies principalement utilis√©es dans ce cours : magrittr, tibble, dplyr, readr, readxls, ggplot2\n\nData frames :\n\nformat des donn√©es dans R\nobservations (=individus) en lignes et variables en colonnes\n\ntibble :\n\nformat tbl_df bas√© sur un data.frame\ntraitement identique aux data.frame\n\nConseils :\n\npr√©f√©rer les librairies d√©velopp√©es par des laboratoires ou des entreprises (mises √† jour susceptibles d‚Äô√™tre plus r√©guli√®res)\n√©crire et sauvegarder ses codes dans un script R ou R Studio\nse r√©f√©rencer √† l‚Äôaide pour l‚Äôutilisation des fonctions"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#les-donn√©es-avec-tibble",
    "href": "talks/Cours manipulation/index.html#les-donn√©es-avec-tibble",
    "title": "Cours de manipulation des donn√©es",
    "section": "Les donn√©es avec tibble",
    "text": "Les donn√©es avec tibble\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nlibrary(tibble)\nas_tibble(mtcars)\n\n# A tibble: 32 √ó 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ‚Ä¶ with 22 more rows"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#importation-de-donn√©es-√†-partir-de-fichiers-externes",
    "href": "talks/Cours manipulation/index.html#importation-de-donn√©es-√†-partir-de-fichiers-externes",
    "title": "Cours de manipulation des donn√©es",
    "section": "Importation de donn√©es √† partir de fichiers externes",
    "text": "Importation de donn√©es √† partir de fichiers externes\n\nImportation de donn√©es √† partir de fihiers externes de divers formats\nImportation √† partir d‚Äôun fichier .txt :\n\n\nlibrary(readr)\niris = read_delim(\"Donnees/iris.txt\", delim=\"\\t\")\niris\n\n# A tibble: 150 √ó 5\n   `Sepal Length` `Sepal Width` `Petal Length` `Petal Width` Species\n            <dbl>         <dbl>          <dbl>         <dbl> <chr>  \n 1            5.1           3.5            1.4           0.2 setosa \n 2            4.9           3              1.4           0.2 setosa \n 3            4.7           3.2            1.3           0.2 setosa \n 4            4.6           3.1            1.5           0.2 setosa \n 5            5             3.6            1.4           0.2 setosa \n 6            5.4           3.9            1.7           0.4 setosa \n 7            4.6           3.4            1.4           0.3 setosa \n 8            5             3.4            1.5           0.2 setosa \n 9            4.4           2.9            1.4           0.2 setosa \n10            4.9           3.1            1.5           0.1 setosa \n# ‚Ä¶ with 140 more rows"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#importation-de-donn√©es-√†-partir-de-fichiers-externes-2",
    "href": "talks/Cours manipulation/index.html#importation-de-donn√©es-√†-partir-de-fichiers-externes-2",
    "title": "Cours de manipulation des donn√©es",
    "section": "Importation de donn√©es √† partir de fichiers externes (2)",
    "text": "Importation de donn√©es √† partir de fichiers externes (2)\n\nImportation √† partir d‚Äôun fichier .xlsx :\n\n\nlibrary(readxl)\niris = read_excel(\"Donnees/iris.xlsx\")\nstr(iris)\n\ntibble [150 √ó 5] (S3: tbl_df/tbl/data.frame)\n $ Sepal Length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal Width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : chr [1:150] \"setosa\" \"setosa\" \"setosa\" \"setosa\" ..."
  },
  {
    "objectID": "talks/Cours manipulation/index.html#importation-de-donn√©es-√†-partir-de-fichiers-externes-3",
    "href": "talks/Cours manipulation/index.html#importation-de-donn√©es-√†-partir-de-fichiers-externes-3",
    "title": "Cours de manipulation des donn√©es",
    "section": "Importation de donn√©es √† partir de fichiers externes (3)",
    "text": "Importation de donn√©es √† partir de fichiers externes (3)\n\nImportation √† partir d‚Äôun fichier SAS :\n\n\nlibrary(haven)\niris = read_sas(\"Donnees/iris.sas7bdat\")\niris\n\n# A tibble: 150 √ó 5\n   Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <chr>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ‚Ä¶ with 140 more rows"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#importation-de-donn√©es-issues-du-web-2",
    "href": "talks/Cours manipulation/index.html#importation-de-donn√©es-issues-du-web-2",
    "title": "Cours de manipulation des donn√©es",
    "section": "Importation de donn√©es issues du Web (2)",
    "text": "Importation de donn√©es issues du Web (2)\n\nLecture de pages Web\n\nExemple\n\n\n\nlibrary(rvest)\nsw = read_html(\"https://www.imdb.com/title/tt0076759/\")"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#manipulation-de-donn√©es",
    "href": "talks/Cours manipulation/index.html#manipulation-de-donn√©es",
    "title": "Cours de manipulation des donn√©es",
    "section": "Manipulation de donn√©es",
    "text": "Manipulation de donn√©es\n\nLe ‚Äúpipe‚Äù %>% (package magrittr) : ctrl+shift+m + permet d‚Äôencha√Æner les op√©rations + √©criture plus naturelle et plus lisible\n\n\nx = c(1,4,2,3,6,10,2,11)\nmean(x)\n\n[1] 4.875\n\nx %>% mean\n\n[1] 4.875\n\n\n\nManipulation de donn√©es (package dplyr) : ajouts de variables, filtres, organisation, etc‚Ä¶"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-2",
    "href": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-2",
    "title": "Cours de manipulation des donn√©es",
    "section": "Manipulation de donn√©es (2)",
    "text": "Manipulation de donn√©es (2)\n\nAjouter √† la table de donn√©es mtcars le nom des mod√®les de voiture contenu dans les noms de lignes de la table (nouvelle variable d√©nomm√©e model) :\n\n\nlibrary(tibble)\ncars = mtcars %>% rownames_to_column(\"model\")\n\n\nFiltrer les observations suivant les valeurs de variables\n\n\n\n\n\nlibrary(dplyr)\ncars %>% filter(mpg>=30)\ncars %>% filter(cyl==4)\ncars %>% filter(cyl==4 & qsec >17)\ncars %>% filter(between(mpg, 30, 32))"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-3",
    "href": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-3",
    "title": "Cours de manipulation des donn√©es",
    "section": "Manipulation de donn√©es (3)",
    "text": "Manipulation de donn√©es (3)\n\nS√©lection de lignes par leurs indices (num√©ros)\n\n\ncars %>% slice(1:2)\ncars %>% slice(c(2,6))\ncars %>% slice(seq(1,n(), by=4))\ncars %>% slice(31:n())\n\no√π \\(n()\\) est le nombre de lignes de la table\n\nS√©lection de variables\n\n\ncars %>% select(model)\ncars %>% select(mpg,cyl,model)\ncars %>% select(2,5,7)\ncars %>% select(starts_with(\"m\"))\n\nVoir ?select_helpers pour la liste des possibilit√©s"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-4",
    "href": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-4",
    "title": "Cours de manipulation des donn√©es",
    "section": "Manipulation de donn√©es (4)",
    "text": "Manipulation de donn√©es (4)\n\nTri selon les valeurs d‚Äôune ou plusieurs variables\n\n\ncars %>% arrange(mpg)\ncars %>% arrange(am,mpg)\ncars %>% arrange(desc(mpg))\n\no√π desc() indique un ordre d√©croissant.\n\nAjout de variables\n\n\ncars %>% mutate(cyl_ratio=cyl/carb)\ncars %>% mutate(cyl_ratio=cyl/carb, wt_ratio=wt/hp)\n\n\nPour garder uniquement la variable cr√©√©e\n\n\ncars %>% transmute(cyl_ratio=cyl/carb)"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-5",
    "href": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-5",
    "title": "Cours de manipulation des donn√©es",
    "section": "Manipulation de donn√©es (5)",
    "text": "Manipulation de donn√©es (5)\n\nSuppression des doublons\n\n\nunique(cars$cyl)\n\n[1] 6 4 8\n\ncars %>% select(cyl) %>% distinct\n\n  cyl\n1   6\n2   4\n3   8"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-6",
    "href": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-6",
    "title": "Cours de manipulation des donn√©es",
    "section": "Manipulation de donn√©es (6)",
    "text": "Manipulation de donn√©es (6)\n\nEncha√Æner les op√©rations\n\n\ncars %>% select(starts_with(\"c\")) %>% summary\n\n      cyl             carb      \n Min.   :4.000   Min.   :1.000  \n 1st Qu.:4.000   1st Qu.:2.000  \n Median :6.000   Median :2.000  \n Mean   :6.188   Mean   :2.812  \n 3rd Qu.:8.000   3rd Qu.:4.000  \n Max.   :8.000   Max.   :8.000  \n\ncars %>% select(mpg,wt,hp) %>% as_tibble %>% head\n\n# A tibble: 6 √ó 3\n    mpg    wt    hp\n  <dbl> <dbl> <dbl>\n1  21    2.62   110\n2  21    2.88   110\n3  22.8  2.32    93\n4  21.4  3.22   110\n5  18.7  3.44   175\n6  18.1  3.46   105"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-7",
    "href": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-7",
    "title": "Cours de manipulation des donn√©es",
    "section": "Manipulation de donn√©es (7)",
    "text": "Manipulation de donn√©es (7)\n\nGrouper les individus en fonction des modalit√©s d‚Äôune variable\n\n\ncars %>% group_by(cyl)\n\n\nExemple : d√©terminer le nombre de mod√®les de la table cars dans chaque cat√©gorie de cylindre\n\n\ncars %>% group_by(cyl) %>% summarise(n=n())\n\n# A tibble: 3 √ó 2\n    cyl     n\n  <dbl> <int>\n1     4    11\n2     6     7\n3     8    14"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-8",
    "href": "talks/Cours manipulation/index.html#manipulation-de-donn√©es-8",
    "title": "Cours de manipulation des donn√©es",
    "section": "Manipulation de donn√©es (8)",
    "text": "Manipulation de donn√©es (8)\n\nJoindre deux tables en fonction des valeurs de l‚Äôune des variables\n\n\nengine = tibble(\n  cyl = c(6,8,12),\n  type = c(\"medium\", \"big\", \"very big\")\n)\n\nExtraction des tables en jointure entre cars et engine :\n\ncars %>% inner_join(engine) \ncars %>% left_join(engine)\ncars %>% right_join(engine)\ncars %>% full_join(engine)\ncars %>% semi_join(engine)\ncars %>% anti_join(engine)"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#tp-exercice-1-avec-tibble-dplyr-et-readr",
    "href": "talks/Cours manipulation/index.html#tp-exercice-1-avec-tibble-dplyr-et-readr",
    "title": "Cours de manipulation des donn√©es",
    "section": "TP : Exercice 1 (avec tibble, dplyr et readr)",
    "text": "TP : Exercice 1 (avec tibble, dplyr et readr)\n\nDonner la liste des diff√©rentes esp√®ces d‚Äôiris dans la table iris\nS√©lectionner\n\nles 60 premiers iris de la table\nles 50 derniers iris de la table\n1 iris sur 10 dans la table\n\nS√©lectionner les iris ayant les plus longues S√©pales et les plus longues P√©tales. En donner la r√©partition par esp√®ce.\nCr√©er une nouvelle table en organisant la table iris par ordre alphab√©tique des esp√®ces et par ordre croissant des longueurs de S√©pale\nCr√©er une nouvelle table en ajoutant 2 colonnes √† la table iris contenant respectivement les rapports longueur/largeur de S√©pale et de P√©tale\nImporter les 4 tables concernant les indicateurs de gouvernance fournis par la banque mondiale, WGIValues.csv, WGICountry.csv, WGISerie.csv et WGIType.csv\nCr√©er une nouvelle table contenant les noms des pays, s√©ries et indicateurs √† la place des codes correspondants"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#donn√©es-sp√©cifiques",
    "href": "talks/Cours manipulation/index.html#donn√©es-sp√©cifiques",
    "title": "Cours de manipulation des donn√©es",
    "section": "Donn√©es sp√©cifiques",
    "text": "Donn√©es sp√©cifiques\n\nGestion de cha√Ænes de caract√®res (package stringr) :\n\n\nlibrary(stringr)\nstr_length(cars$model)\n\n [1]  9 13 10 14 17  7 10  9  8  8  9 10 10 11 18 19 17  8 11 14 13 16 11 10 16\n[26]  9 13 12 14 12 13 10\n\nstr_c(cars$model, collapse = \", \")\n\n[1] \"Mazda RX4, Mazda RX4 Wag, Datsun 710, Hornet 4 Drive, Hornet Sportabout, Valiant, Duster 360, Merc 240D, Merc 230, Merc 280, Merc 280C, Merc 450SE, Merc 450SL, Merc 450SLC, Cadillac Fleetwood, Lincoln Continental, Chrysler Imperial, Fiat 128, Honda Civic, Toyota Corolla, Toyota Corona, Dodge Challenger, AMC Javelin, Camaro Z28, Pontiac Firebird, Fiat X1-9, Porsche 914-2, Lotus Europa, Ford Pantera L, Ferrari Dino, Maserati Bora, Volvo 142E\"\n\nstr_sub(cars$model, 1, 5)\n\n [1] \"Mazda\" \"Mazda\" \"Datsu\" \"Horne\" \"Horne\" \"Valia\" \"Duste\" \"Merc \" \"Merc \"\n[10] \"Merc \" \"Merc \" \"Merc \" \"Merc \" \"Merc \" \"Cadil\" \"Linco\" \"Chrys\" \"Fiat \"\n[19] \"Honda\" \"Toyot\" \"Toyot\" \"Dodge\" \"AMC J\" \"Camar\" \"Ponti\" \"Fiat \" \"Porsc\"\n[28] \"Lotus\" \"Ford \" \"Ferra\" \"Maser\" \"Volvo\""
  },
  {
    "objectID": "talks/Cours manipulation/index.html#donn√©es-sp√©cifiques-2",
    "href": "talks/Cours manipulation/index.html#donn√©es-sp√©cifiques-2",
    "title": "Cours de manipulation des donn√©es",
    "section": "Donn√©es sp√©cifiques (2)",
    "text": "Donn√©es sp√©cifiques (2)\n\nGestion des expressions r√©guli√®res (package stringr) :\n\n\nstr_subset(cars$model, \"Merc\")\nstr_subset(cars$model, \"[0-9]\")\nstr_detect(cars$model, \"[0-9]\")\nstr_match(cars$model, \"(.+)[ ](.+)\")\nstr_split(cars$model, \" \")"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#tp-exercice-2-avec-stringr",
    "href": "talks/Cours manipulation/index.html#tp-exercice-2-avec-stringr",
    "title": "Cours de manipulation des donn√©es",
    "section": "TP : Exercice 2 (avec stringr)",
    "text": "TP : Exercice 2 (avec stringr)\n\nCr√©er le vecteur x compos√© des \\(5\\) cha√Ænes de caract√®res suivantes : ‚ÄúJe‚Äù ‚Äúsuis‚Äù ‚Äúen‚Äù ‚ÄúDUT‚Äù ‚ÄúSTID‚Äù\nAjouter la cha√Æne ‚Äúalternance‚Äù √† la fin du vecteur x\nDonner le nombre de caract√®res composant chaque √©l√©ment de x\nAjouter une majuscule √† ‚Äúalternance‚Äù (indication : str_to_title)\nTrouver tous les √©l√©ments de x contenant le caract√®re ‚Äúe‚Äù et les extraire.\nConcat√©ner les √©l√©ments de x afin de ne former qu‚Äôune seule phrase. Combien de caract√®res contient ce nouvel objet ? Interpr√©ter.\nRe-d√©couper l‚Äôobjet obtenu en cha√Ænes de caract√®res distinctes"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#dates",
    "href": "talks/Cours manipulation/index.html#dates",
    "title": "Cours de manipulation des donn√©es",
    "section": "Dates",
    "text": "Dates\n\nGestion des dates ais√©e (package lubridate)\n\n\nlibrary(lubridate)\nnow()\n\n[1] \"2022-11-12 12:36:25 CET\"\n\ntoday = today()\ntoday\n\n[1] \"2022-11-12\"\n\nyear(today)\n\n[1] 2022"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#dates-2",
    "href": "talks/Cours manipulation/index.html#dates-2",
    "title": "Cours de manipulation des donn√©es",
    "section": "Dates (2)",
    "text": "Dates (2)\n\nmonth(today)\nmonth(today, label = TRUE)\nmonth(today, label = TRUE, abbr = FALSE)\nday(today)\nmday(today)\nwday(today)\nwday(today, label = TRUE)\nwday(today, label = TRUE, week_start = 1)\nwday(today, week_start = 1)\nyday(today)"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#dates-3",
    "href": "talks/Cours manipulation/index.html#dates-3",
    "title": "Cours de manipulation des donn√©es",
    "section": "Dates (3)",
    "text": "Dates (3)\n\ntoday + period(week = 1, day = 3)\n\n[1] \"2022-11-22\"\n\ntoday + period(\"1W 3D\")\n\n[1] \"2022-11-22\"\n\ntoday - years(1) - days(10)\n\n[1] \"2021-11-02\""
  },
  {
    "objectID": "talks/Cours manipulation/index.html#dates-4",
    "href": "talks/Cours manipulation/index.html#dates-4",
    "title": "Cours de manipulation des donn√©es",
    "section": "Dates (4)",
    "text": "Dates (4)\n\nCalculer des p√©riodes en secondes ou en ann√©es (par d√©faut en jours) :\n\n\nbday = ymd(\"19771114\")\ndiff = today - bday\ndiff\n\nTime difference of 16434 days\n\nas.period(diff)\n\n[1] \"16434d 0H 0M 0S\"\n\nas.duration(diff)\n\n[1] \"1419897600s (~44.99 years)\""
  },
  {
    "objectID": "talks/Cours manipulation/index.html#tp-exercice-3-avec-tibble-dplyr-stringr",
    "href": "talks/Cours manipulation/index.html#tp-exercice-3-avec-tibble-dplyr-stringr",
    "title": "Cours de manipulation des donn√©es",
    "section": "TP : Exercice 3 (avec tibble, dplyr, stringr)",
    "text": "TP : Exercice 3 (avec tibble, dplyr, stringr)\n\nAfficher le jeu de donn√©es starwars (package dplyr)\nDonner le nombre total de personnages distincts apparaissant dans les trilogies ‚ÄúStar Wars‚Äù (indication : nrow)\nDonner le pourcentage de valeurs manquantes par variable (indication : is.na, colSums)\nFaire de m√™me avec les personnages (indication : rowSums)\nLister les diff√©rentes races des personnages de la table starwars. Combien de races apparaissent dans les trilogies ‚ÄúStar Wars‚Äù ?\nDonner le nombre de personnages dans chaque race, en triant par ordre d√©croissant de ce nombre\nChoisir 2 races pour lesquelles on d√©nombre plus de 2 personnages et donner le nom de ces personnages. Quels sont les personnages dont la race est ind√©finie ? (indication : is.na)\nOptionnel : donner les noms des personnages ayant chang√© de couleur de cheveux (indication : regarder la variable hair_color et utiliser la librairie stringr)\nOptionnel : quels personnages apparaissent dans plus de 4 films ? (indication : bind_cols, add_column)"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#statistiques-sous-r-quelques-exemples",
    "href": "talks/Cours manipulation/index.html#statistiques-sous-r-quelques-exemples",
    "title": "Cours de manipulation des donn√©es",
    "section": "Statistiques sous R : quelques exemples",
    "text": "Statistiques sous R : quelques exemples\n\nR√©sumer une ou plusieurs variables dans un data.frame :\n\n\ncars %>% summarise(n=n(), mpg_av=mean(mpg), \n                   wt_med=median(wt))\n\n   n   mpg_av wt_med\n1 32 20.09062  3.325\n\n\n\nR√©sumer toutes les variables dans un data.frame suivant le m√™me indicateur statistique :\n\n\nmtcars %>% summarise_all(mean)\n\n       mpg    cyl     disp       hp     drat      wt     qsec     vs      am\n1 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 0.4375 0.40625\n    gear   carb\n1 3.6875 2.8125"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#statistiques-sous-r-quelques-exemples-2",
    "href": "talks/Cours manipulation/index.html#statistiques-sous-r-quelques-exemples-2",
    "title": "Cours de manipulation des donn√©es",
    "section": "Statistiques sous R : quelques exemples (2)",
    "text": "Statistiques sous R : quelques exemples (2)\n\nR√©sumer par groupes d√©finis par les modalit√©s d‚Äôune variable :\n\n\ncars %>% group_by(cyl) %>% \n  summarise(n=n(), mpg_av=mean(mpg), wt_med=median(wt))\n\n# A tibble: 3 √ó 4\n    cyl     n mpg_av wt_med\n  <dbl> <int>  <dbl>  <dbl>\n1     4    11   26.7   2.2 \n2     6     7   19.7   3.22\n3     8    14   15.1   3.76"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#statistiques-sous-r-quelques-exemples-2-1",
    "href": "talks/Cours manipulation/index.html#statistiques-sous-r-quelques-exemples-2-1",
    "title": "Cours de manipulation des donn√©es",
    "section": "Statistiques sous R : quelques exemples (2)",
    "text": "Statistiques sous R : quelques exemples (2)\n\nUtilisation des fonctions descriptives classiques\n\n\ncars %>% select(mpg,wt,qsec) %>% colMeans\n\n     mpg       wt     qsec \n20.09062  3.21725 17.84875 \n\ncars %>% select(mpg,wt,qsec) %>% cor\n\n            mpg         wt       qsec\nmpg   1.0000000 -0.8676594  0.4186840\nwt   -0.8676594  1.0000000 -0.1747159\nqsec  0.4186840 -0.1747159  1.0000000"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#statistiques-sous-r-quelques-exemples-3",
    "href": "talks/Cours manipulation/index.html#statistiques-sous-r-quelques-exemples-3",
    "title": "Cours de manipulation des donn√©es",
    "section": "Statistiques sous R : quelques exemples (3)",
    "text": "Statistiques sous R : quelques exemples (3)\n\nCalculs d‚Äôagr√©gats sur toutes les variables\n\n\ncars %>% group_by(cyl) %>% \n  select(cyl,mpg,wt,qsec) %>% \n  summarise_all(mean)\n\n# A tibble: 3 √ó 4\n    cyl   mpg    wt  qsec\n  <dbl> <dbl> <dbl> <dbl>\n1     4  26.7  2.29  19.1\n2     6  19.7  3.12  18.0\n3     8  15.1  4.00  16.8"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#statistiques-sous-r-quelques-exemples-4",
    "href": "talks/Cours manipulation/index.html#statistiques-sous-r-quelques-exemples-4",
    "title": "Cours de manipulation des donn√©es",
    "section": "Statistiques sous R : quelques exemples (4)",
    "text": "Statistiques sous R : quelques exemples (4)\n\nProduction de graphiques avec les fonctions de base\n\n\ncars %>% select(mpg,wt) %>% \n  plot(main=\"Weight vs Miles/gallon\")"
  },
  {
    "objectID": "talks/Cours manipulation/index.html#tp-exercice-4-avec-tibble-dplyr",
    "href": "talks/Cours manipulation/index.html#tp-exercice-4-avec-tibble-dplyr",
    "title": "Cours de manipulation des donn√©es",
    "section": "TP : Exercice 4 (avec tibble, dplyr)",
    "text": "TP : Exercice 4 (avec tibble, dplyr)\n\nAfficher les indicateurs statistiques r√©sumant les donn√©es iris\nCr√©er deux tables r√©sumant ces m√™mes donn√©es dont les valeurs sont, par esp√®ce, le nombre d‚Äôiris, les moyennes et les m√©dianes de chaque variable, l‚Äôune concernant les S√©pales, l‚Äôautre les P√©tales\nCr√©er une seule table rassemblant toutes les informations pr√©c√©dentes\nRepr√©senter la distribution des esp√®ces sous forme de diagramme en barres et de diagramme circulaire (indication : table, barplot, pie)\nRepr√©senter le lien entre la couleur des yeux et la couleur des cheveux des personnages de la base de donn√©es starwars (indication : table, barplot)"
  },
  {
    "objectID": "talks/Cours visualisation/index.html",
    "href": "talks/Cours visualisation/index.html",
    "title": "Cours de visualisation des donn√©es",
    "section": "",
    "text": "Permet une repr√©sentation des donn√©es plus ais√©e et plus lisible lorsque l‚Äôon traite plus de 2 variables.\nAdapt√©e au format tibble, sous-librairie de tidyverse.\nConstruite sur une grammaire superpos√©e, bas√©e sur la grammaire des graphiques propos√©e par Wilkinson en 2005.\nGraphiques construits en couches ind√©pendantes, √† inclure selon la repr√©sentation souhait√©e :\n\nles donn√©es √† repr√©senter ainsi que leurs attributs esth√©tiques (axes, taille, couleurs, symboles, ‚Ä¶)\nles attributs g√©om√©triques (points, lignes, barres, ‚Ä¶)\nles transformations statistiques (d√©nombrement, tendances, ‚Ä¶)\nles √©chelles\nle syst√®me de coordonn√©es (cart√©sien, logarithmique, ‚Ä¶)\nle d√©coupage en facettes (plusieurs graphiques dans la m√™me fen√™tre).\n\nValeurs par d√©faut: par exemple, couche transformation statistique effectu√©e par d√©faut si la couche attributs g√©om√©triques est ‚Äúboxplot‚Äù."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#ggplot2-fonctions-et-couches",
    "href": "talks/Cours visualisation/index.html#ggplot2-fonctions-et-couches",
    "title": "Cours de visualisation des donn√©es",
    "section": "ggplot2 : fonctions et couches",
    "text": "ggplot2 : fonctions et couches\n\nggplot() : fonction principale, cr√©e et renvoie le graphique, en d√©finissant quelles donn√©es sont repr√©sent√©es.\naes() : d√©finit les attributs esth√©tiques.\ngeom_xxx() : d√©finit la couche attributs g√©om√©triques.\nstat_xxx() : d√©finit la couche transformations statistiques.\nscale_xxx() : d√©finit la couche √©chelles.\ncoord_xxx() : d√©finit la couche syst√®me de coordonn√©es.\nfacet_xxx() : d√©finit la couche d√©coupage en facettes.\ntheme_xxx(), labs(), xlab(), ylab(), ggtitle() : pour personnaliser le graphique.\nSeule aes() s‚Äôutilise √† l‚Äôint√©rieur des autres fonctions, toutes les couches √©tant g√©r√©es de mani√®re ind√©pendante.\nPossibilit√© d‚Äôenregistrer la sortie de ggplot() dans une variable, puis de lui ajouter des couches. Appeler la variable pour afficher le graphique."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#repr√©sentation-en-couches",
    "href": "talks/Cours visualisation/index.html#repr√©sentation-en-couches",
    "title": "Cours de visualisation des donn√©es",
    "section": "Repr√©sentation en couches",
    "text": "Repr√©sentation en couches\n\nCouche principale : avec ggplot() et aes(), d√©finition des donn√©es √† repr√©senter et des axes de coordonn√©es.\nExemple avec les donn√©es diamonds :\n\n\nlibrary(ggplot2)\nggplot(diamonds, aes(carat))"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#repr√©sentation-en-couches-2",
    "href": "talks/Cours visualisation/index.html#repr√©sentation-en-couches-2",
    "title": "Cours de visualisation des donn√©es",
    "section": "Repr√©sentation en couches (2)",
    "text": "Repr√©sentation en couches (2)\n\nCouche attributs g√©om√©triques : ajout de la repr√©sentation souhait√©e pour les donn√©es d√©finies dans ggplot().\nExemple : ajout de l‚Äôhistogramme de la variable carat :\n\n\nggplot(diamonds, aes(carat)) + geom_histogram()\n\n\n\n\n\nRemarque : dans cet exemple, la couche transformations statistiques est cach√©e, correspondant au calcul des classes, amplitudes et densit√©s de l‚Äôhistogramme."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#repr√©sentation-en-couches-3",
    "href": "talks/Cours visualisation/index.html#repr√©sentation-en-couches-3",
    "title": "Cours de visualisation des donn√©es",
    "section": "Repr√©sentation en couches (3)",
    "text": "Repr√©sentation en couches (3)\n\nStockage du r√©sultat dans une variable, √† laquelle on peut ajouter des couches\n\n\ng = ggplot(diamonds, aes(carat))\ng+geom_histogram()"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#repr√©sentation-en-couches-4",
    "href": "talks/Cours visualisation/index.html#repr√©sentation-en-couches-4",
    "title": "Cours de visualisation des donn√©es",
    "section": "Repr√©sentation en couches (4)",
    "text": "Repr√©sentation en couches (4)\n\nStockage du r√©sultat dans une variable, √† laquelle on peut ajouter des couches\n\n\ng = ggplot(diamonds, aes(carat))+geom_histogram()\ng"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#personnalisation",
    "href": "talks/Cours visualisation/index.html#personnalisation",
    "title": "Cours de visualisation des donn√©es",
    "section": "Personnalisation",
    "text": "Personnalisation\n\nAjout d‚Äôun titre au graphique avec ggtitle()\n\n\ng = ggplot(diamonds, aes(carat))+geom_histogram()\ng+ggtitle(\"Distribution des carats\")"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#personnalisation-2",
    "href": "talks/Cours visualisation/index.html#personnalisation-2",
    "title": "Cours de visualisation des donn√©es",
    "section": "Personnalisation (2)",
    "text": "Personnalisation (2)\n\nChangement du th√®me par d√©faut en classique avec theme_classic()\n\n\ng = ggplot(diamonds, aes(carat))+geom_histogram()\ng+ggtitle(\"Distribution des carats\")+theme_classic()"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#personnalisation-3",
    "href": "talks/Cours visualisation/index.html#personnalisation-3",
    "title": "Cours de visualisation des donn√©es",
    "section": "Personnalisation (3)",
    "text": "Personnalisation (3)\n\nChangement du th√®me par d√©faut sans le fond gris avec theme_light()\n\n\ng = ggplot(diamonds, aes(carat))+geom_histogram()\ng+ggtitle(\"Distribution des carats\")+theme_light()\n\n\n\n\n\nAutres th√®mes disponibles dans la librairie (voir ?theme_light)"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#personnalisation-4",
    "href": "talks/Cours visualisation/index.html#personnalisation-4",
    "title": "Cours de visualisation des donn√©es",
    "section": "Personnalisation (4)",
    "text": "Personnalisation (4)\n\nGestion du nom de l‚Äôaxe horizontal (abcisses) avec xlab()\n\n\ng = ggplot(diamonds, aes(carat))+geom_histogram()\ng+ggtitle(\"Distribution des carats\")+xlab(\"Carats\")"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#personnalisation-5",
    "href": "talks/Cours visualisation/index.html#personnalisation-5",
    "title": "Cours de visualisation des donn√©es",
    "section": "Personnalisation (5)",
    "text": "Personnalisation (5)\n\nGestion du nom de l‚Äôaxe vertical (ordonn√©es) avec ylab()\n\n\ng = ggplot(diamonds, aes(carat))+geom_histogram()\ng+ggtitle(\"Distribution des carats\")+ylab(\"Effectif\")"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#personnalisation-6",
    "href": "talks/Cours visualisation/index.html#personnalisation-6",
    "title": "Cours de visualisation des donn√©es",
    "section": "Personnalisation (6)",
    "text": "Personnalisation (6)\n\nGestion du titre et des noms des axes avec labs()\n\n\ng = ggplot(diamonds, aes(carat))+geom_histogram()\ng+labs(title=\"Distribution des carats\", \n       x=\"Carats\", y=\"Effectif\")\n\n\n\n\n\nPersonnalisation de la position, taille, couleur, etc‚Ä¶ d‚Äôun nom ou titre avec theme()"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#aspect-suivant-une-variable",
    "href": "talks/Cours visualisation/index.html#aspect-suivant-une-variable",
    "title": "Cours de visualisation des donn√©es",
    "section": "Aspect suivant une variable",
    "text": "Aspect suivant une variable\n\nExemple : cr√©ation d‚Äôune barre de comptage avec geom_bar()\n\n\nggplot(diamonds, aes(x=\"\"))+geom_bar()"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#aspect-suivant-une-variable-2",
    "href": "talks/Cours visualisation/index.html#aspect-suivant-une-variable-2",
    "title": "Cours de visualisation des donn√©es",
    "section": "Aspect suivant une variable (2)",
    "text": "Aspect suivant une variable (2)\n\nExemple : remplissage de la barre avec des couleurs suivant les modalit√©s de la variable cut\n\n\nggplot(diamonds, aes(x=\"\", fill=cut))+geom_bar()\n\n\n\n\n\n\n\n\n\nRemarque : l√©gendes par d√©faut, peuvent √™tre personnalis√©es (voir suite du cours)"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#tp-exercice-1",
    "href": "talks/Cours visualisation/index.html#tp-exercice-1",
    "title": "Cours de visualisation des donn√©es",
    "section": "TP : Exercice 1",
    "text": "TP : Exercice 1\n\nRepr√©senter l‚Äôhistogramme de la distribution de la variable Sepal.Length du jeu de donn√©es iris avec les valeurs par d√©faut.\nChanger le nombre de classes (=bins) souhait√©es dans l‚Äôhistogramme afin d‚Äôobtenir une distribution moins variable (indication : options bins ou binwidth dans la couche geom_histogram).\nChanger le th√®me afin d‚Äôobtenir un graphique sans fond gris.\nAjouter un titre et des noms adapt√©s aux axes sur le graphique obtenu.\nRepr√©senter la distribution des esp√®ces sous forme de barre empil√©e. Ajouter un titre et enlever le nom de l‚Äôaxe des abcisses.\nRepr√©senter la distribution de la variable Sepal.Length conditionnellement √† l‚Äôesp√®ce sous forme de bo√Ætes √† moustaches (indication : geom_boxplot). Ajouter un titre et changer les noms des axes."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#changement-d√©chelle",
    "href": "talks/Cours visualisation/index.html#changement-d√©chelle",
    "title": "Cours de visualisation des donn√©es",
    "section": "Changement d‚Äô√©chelle",
    "text": "Changement d‚Äô√©chelle\n\nChangement d‚Äô√©chelle en utilisant une formule dans aes(),\nChangement d‚Äô√©chelle en utilisant les variables cr√©es par les fonctions du type geom_xxx(), de type ..xxx..,\nChangement d‚Äô√©chelle en ajoutant une couche avec les fonctions du type scale_aspect esth√©tique √† modifier_type de l‚Äôaspect().\nExemple : changer l‚Äôeffectif en fr√©quence ou en pourcentage dans un diagramme."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#changement-d√©chelle-2",
    "href": "talks/Cours visualisation/index.html#changement-d√©chelle-2",
    "title": "Cours de visualisation des donn√©es",
    "section": "Changement d‚Äô√©chelle (2)",
    "text": "Changement d‚Äô√©chelle (2)\n\nExemple : changer l‚Äôeffectif en fr√©quence dans le diagramme pr√©c√©dent en utilisant la variable ..count.. cr√©√©e par geom_bar()\n\n\nggplot(diamonds, aes(x=\"\", fill=cut))+\n  geom_bar(aes(y=..count../sum(..count..)))"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#changement-d√©chelle-3",
    "href": "talks/Cours visualisation/index.html#changement-d√©chelle-3",
    "title": "Cours de visualisation des donn√©es",
    "section": "Changement d‚Äô√©chelle (3)",
    "text": "Changement d‚Äô√©chelle (3)\n\nRemarque : option position=‚Äúfill‚Äù dans geom_bar() pour le changement en fr√©quence \\(\\Rightarrow\\) m√™me r√©sultat.\n\n\nggplot(diamonds, aes(x=\"\", fill=cut))+\n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#changement-d√©chelle-4",
    "href": "talks/Cours visualisation/index.html#changement-d√©chelle-4",
    "title": "Cours de visualisation des donn√©es",
    "section": "Changement d‚Äô√©chelle (4)",
    "text": "Changement d‚Äô√©chelle (4)\n\nChanger l‚Äôeffectif en fr√©quence, puis la fr√©quence en pourcentage : fonction scale_y_continuous() (o√π percent() est charg√© via la librairie scales)\n\n\nlibrary(scales)\nggplot(diamonds, aes(x=\"\", fill=cut))+\n  geom_bar(position = \"fill\")+\n  scale_y_continuous(labels = percent)"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#changement-de-coordonn√©es",
    "href": "talks/Cours visualisation/index.html#changement-de-coordonn√©es",
    "title": "Cours de visualisation des donn√©es",
    "section": "Changement de coordonn√©es",
    "text": "Changement de coordonn√©es\n\nSuivant le type de graphique souhait√©, il peut √™tre n√©cessaire de changer le type de coordonn√©es repr√©sent√©es.\nExemples typiques:\n\ncoordonn√©es cart√©siennes (x,y) pour repr√©senter une variable en fonction d‚Äôune autre,\ncoordonn√©es logarithmiques (x, log(y)) pour repr√©senter des concentrations,\ncoordonn√©es polaires (xcos(y), xsin(y)) pour effectuer des diagrammes circulaires.\n\nPar d√©faut, ggplot() repr√©sente les variables en coordonn√©es cart√©siennes.\nChangement de coordonn√©es effectu√© en ajoutant une couche avec les fonctions du type coord_xxx()."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#changement-de-coordonn√©es-2",
    "href": "talks/Cours visualisation/index.html#changement-de-coordonn√©es-2",
    "title": "Cours de visualisation des donn√©es",
    "section": "Changement de coordonn√©es (2)",
    "text": "Changement de coordonn√©es (2)\n\nExemple : repr√©sentation de la distribution de la variable cut sous forme de diagramme circulaire avec la fonction coord_polar() (option width=1 pour enlever le trou au centre du diagramme)\n\n\nggplot(diamonds, aes(x = \"\", fill = cut)) + \n    geom_bar(position = \"fill\", width = 1) +\n    scale_y_continuous(labels = percent) +\n    coord_polar(theta = \"y\")"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#changement-de-coordonn√©es-3",
    "href": "talks/Cours visualisation/index.html#changement-de-coordonn√©es-3",
    "title": "Cours de visualisation des donn√©es",
    "section": "Changement de coordonn√©es (3)",
    "text": "Changement de coordonn√©es (3)\n\nExemple de code complet pour un diagramme circulaire propre :\n\n\nggplot(diamonds, aes(x = \"\", fill = cut)) + \n    geom_bar(position = \"fill\", width = 1) +\n    scale_fill_brewer(palette = \"Set2\") +\n    scale_y_continuous(labels = percent) +\n    coord_polar(theta = \"y\") +\n    theme_minimal() +\n    theme(axis.title = element_blank()) +\n    labs(fill = \"Variable cut\")\n\n\no√π :\n\nscale_fill_brewer(palette=xxx) change les couleurs de remplissage,\ntheme(axis.title=element_blank()) enl√®ve les noms des axes,\nlabs(fill=xxx) change le nom de la l√©gende."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#changement-de-coordonn√©es-4",
    "href": "talks/Cours visualisation/index.html#changement-de-coordonn√©es-4",
    "title": "Cours de visualisation des donn√©es",
    "section": "Changement de coordonn√©es (4)",
    "text": "Changement de coordonn√©es (4)\n\nR√©sultat du code pr√©c√©dent :"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#tp-exercice-2-avec-ggplot2-et-scales",
    "href": "talks/Cours visualisation/index.html#tp-exercice-2-avec-ggplot2-et-scales",
    "title": "Cours de visualisation des donn√©es",
    "section": "TP : Exercice 2 avec ggplot2 et scales",
    "text": "TP : Exercice 2 avec ggplot2 et scales\n\nReprendre l‚Äôhistogramme cr√©√© dans l‚Äôexercice 1. Changer l‚Äô√©chelle des ordonn√©es afin d‚Äôavoir une repr√©sentation en densit√© (indication : variable ..density.. en sortie de la couche geom_histogram). Ajouter un titre et changer le nom des axes.\nRepr√©senter sur le m√™me graphique un histogramme par esp√®ce.\nRepr√©senter la distribution des esp√®ces sous forme de diagramme en barres horizontales (indication : coord_flip()). Ajouter un titre et changer les noms des axes.\nRefaire le m√™me graphique avec une distribution en pourcentages.\nRepr√©senter la distribution des esp√®ces sous forme de diagramme circulaire avec les valeurs par d√©faut.\nRefaire le graphique pr√©c√©dent avec une repr√©sentation propre et un titre."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#superposition-de-graphiques",
    "href": "talks/Cours visualisation/index.html#superposition-de-graphiques",
    "title": "Cours de visualisation des donn√©es",
    "section": "Superposition de graphiques",
    "text": "Superposition de graphiques\n\nAis√©e gr√¢ce √† la grammaire ordonn√©e et la repr√©sentation en couches\nExemple : nuage de points des variables carat et price avec geom_point(), et ajout des valeurs sur les axes avec geom_rug()\n\n\ng = ggplot(diamonds, aes(carat, price))+geom_point()\ng+geom_rug()"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#superposition-de-graphiques-2",
    "href": "talks/Cours visualisation/index.html#superposition-de-graphiques-2",
    "title": "Cours de visualisation des donn√©es",
    "section": "Superposition de graphiques (2)",
    "text": "Superposition de graphiques (2)\n\nExemple : ajout d‚Äôune courbe de r√©gression avec geom_smooth()\n\n\ng = ggplot(diamonds, aes(carat, price))+geom_point()\ng+geom_rug()+geom_smooth()"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#superposition-de-graphiques-3",
    "href": "talks/Cours visualisation/index.html#superposition-de-graphiques-3",
    "title": "Cours de visualisation des donn√©es",
    "section": "Superposition de graphiques (3)",
    "text": "Superposition de graphiques (3)\n\nExemple : ajouter la droite de r√©gression avec geom_smooth() (option se=FALSE pour enlever l‚Äôintervalle de pr√©diction)\n\n\ng = ggplot(diamonds, aes(carat, price))+geom_point()\ng+geom_smooth(method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#superposition-de-graphiques-4",
    "href": "talks/Cours visualisation/index.html#superposition-de-graphiques-4",
    "title": "Cours de visualisation des donn√©es",
    "section": "Superposition de graphiques (4)",
    "text": "Superposition de graphiques (4)\n\nRepr√©senter les points suivant les valeurs d‚Äôune variable avec shape et col\nExemple : nuage de points et courbe de r√©gression avec des formes et des couleurs suivant les valeurs de la variable cut\n\n\ng = ggplot(diamonds, aes(carat, price, shape=cut, col=cut))+geom_point()\ng+geom_smooth(se = FALSE)"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#facettes",
    "href": "talks/Cours visualisation/index.html#facettes",
    "title": "Cours de visualisation des donn√©es",
    "section": "Facettes",
    "text": "Facettes\n\nAfin de comparer diff√©rents graphiques, il peut √™tre plus judicieux d‚Äôutiliser les facettes avec facet_wrap() et facet_grid()\nExemple : graphique pr√©c√©dent avec un graphique par valeur de la variable cut\n\n\ng = ggplot(diamonds, aes(carat, price))+geom_point()\ng+geom_smooth(se=FALSE)+facet_wrap(facets=~cut)"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#facettes-2",
    "href": "talks/Cours visualisation/index.html#facettes-2",
    "title": "Cours de visualisation des donn√©es",
    "section": "Facettes (2)",
    "text": "Facettes (2)\n\nR√©glage de la position des facettes avec les options ncol= et nrow=\n\n\ng = ggplot(diamonds, aes(carat, price))+geom_point()\ng+geom_smooth(se=FALSE)+facet_wrap(facets=~cut, ncol=2)"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#facettes-3",
    "href": "talks/Cours visualisation/index.html#facettes-3",
    "title": "Cours de visualisation des donn√©es",
    "section": "Facettes (3)",
    "text": "Facettes (3)\n\nfacet_wrap() : r√©glage automatique de la position des facettes en fonction du nombre de modalit√©s,\nfacet_grid() : r√©partition sur une seule ligne, ou une seule colonne.\nExemple : repr√©sentation en ligne des bo√Ætes √† moustaches de carat suivant les modalit√©s de la variable color, et ce pour chaque modalit√© de la variable cut\n\n\ng = ggplot(diamonds, aes(color, carat))+geom_boxplot()\ng+facet_grid(facets=~cut)"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#facettes-4",
    "href": "talks/Cours visualisation/index.html#facettes-4",
    "title": "Cours de visualisation des donn√©es",
    "section": "Facettes (4)",
    "text": "Facettes (4)\n\nRepr√©sentation en colonne des histogrammes de carat suivant les modalit√©s de la variable cut\n\n\ng = ggplot(diamonds, aes(carat))+geom_histogram()\ng+facet_grid(facets=cut~.)"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#facettes-5",
    "href": "talks/Cours visualisation/index.html#facettes-5",
    "title": "Cours de visualisation des donn√©es",
    "section": "Facettes (5)",
    "text": "Facettes (5)\n\nGestion des √©chelles sur chaque facette : laisser libre l‚Äô√©chelle des ordonn√©es avec scales=‚Äúfree_y‚Äù\n\n\ng = ggplot(diamonds, aes(carat))+geom_histogram()\ng+facet_grid(facets=cut~., scales=\"free_y\")"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#facettes-6",
    "href": "talks/Cours visualisation/index.html#facettes-6",
    "title": "Cours de visualisation des donn√©es",
    "section": "Facettes (6)",
    "text": "Facettes (6)\n\nPossibilit√© de faire une graphique par couple de modalit√©s\nExemple : un nuage de point de price en fonction de carat, avec la droite de r√©gression, et ce pour chaque modalit√© de la variable color (en lignes) et de la variable cut (en colonnes)\n\n\ng = ggplot(diamonds, aes(carat, price)) + geom_point() +\n    geom_smooth(method = \"lm\", fullrange = T)\ng+facet_grid(facets = color ~ cut)"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#facettes-7",
    "href": "talks/Cours visualisation/index.html#facettes-7",
    "title": "Cours de visualisation des donn√©es",
    "section": "Facettes (7)",
    "text": "Facettes (7)\n\nR√©sultat du code pr√©c√©dent"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#tp-exercice-3-avec-ggplot2-et-scales",
    "href": "talks/Cours visualisation/index.html#tp-exercice-3-avec-ggplot2-et-scales",
    "title": "Cours de visualisation des donn√©es",
    "section": "TP : Exercice 3 avec ggplot2 et scales",
    "text": "TP : Exercice 3 avec ggplot2 et scales\n\nRepr√©senter le nuage de points des longueurs de S√©pales en fonction de leurs largeurs. Ajouter la courbe de r√©gression.\nRefaire la question pr√©c√©dente avec, sur le m√™me graphique, des formes et des couleurs de points diff√©rents pour chaque esp√®ce. Ajouter les droites de r√©gression et les indicateurs de valeurs par esp√®ce.\nS√©parer le graphique pr√©c√©dent en facettes, avec un graphique par esp√®ces.\n\nReprendre l‚Äôhistogramme en densit√© cr√©√© dans l‚Äôexercice 2. Repr√©senter sur une colonne un histogramme par esp√®ce d‚Äôiris."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#changements-dans-la-l√©gende",
    "href": "talks/Cours visualisation/index.html#changements-dans-la-l√©gende",
    "title": "Cours de visualisation des donn√©es",
    "section": "Changements dans la l√©gende",
    "text": "Changements dans la l√©gende\n\nChangement de position : couche theme(legend.position = xxx) avec xxx = ‚Äúleft‚Äù, ‚Äútop‚Äù, ‚Äúright‚Äù, ‚Äúbottom‚Äù, ‚Äúnone‚Äù (supprime la l√©gende).\nChangement de titre : couche labs(xxx=nom) s‚Äôil existe une variable pour xxx, o√π xxx=fill, color, etc‚Ä¶ Suppression du titre avec nom = element_blank().\nChangement de l‚Äôordre des modalit√©s : couche scale_xxx_discrete(limits = c(yyy)), o√π\n\nxxx=x si modalit√©s en abcisse et xxx=y si modalit√©s en ordonn√©e,\nyyy nouvel ordre d‚Äôapparition des modalit√©s.\n\nChangement des modalit√©s dans la l√©gende : couche scale_xxx_hue(labels = ) s‚Äôil existe une variable pour xxx, o√π xxx=fill, color, etc‚Ä¶."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#gestion-des-tables-complexes-avec-gather",
    "href": "talks/Cours visualisation/index.html#gestion-des-tables-complexes-avec-gather",
    "title": "Cours de visualisation des donn√©es",
    "section": "Gestion des tables complexes avec gather",
    "text": "Gestion des tables complexes avec gather\n\nFonction gather() (librairie tidyr) : cr√©e une table longue en d√©ployant la table de donn√©es (=table large) sur 2 colonnes (key,value), o√π key est le nom de la variable, et value sa valeur pour chaque observation.\nExemple : d√©ploiement de la table diamonds sur les variables carat, price, depth et cut\n\n\nlibrary(dplyr)\nlibrary(tidyr)\ndiamonds %>% select(carat, price, depth, cut) %>% \n  gather()\n\n\nPassage d‚Äôune table \\(nobs\\times nvar\\) √† une table \\(nobs*nvar \\times 2\\).\nExemple : garder les valeurs de carat pour chaque observation :\n\n\ndiamonds %>% select(carat, price, depth, cut) %>% \n  gather(-carat, key=\"key\", value=\"value\")"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#gestion-des-tables-complexes-avec-gather-2",
    "href": "talks/Cours visualisation/index.html#gestion-des-tables-complexes-avec-gather-2",
    "title": "Cours de visualisation des donn√©es",
    "section": "Gestion des tables complexes avec gather (2)",
    "text": "Gestion des tables complexes avec gather (2)\n\nExemple : Repr√©sentation de price et depth en fonction de carat\n\n\n\n\n\ndiamonds %>% select(carat,price,depth) %>% \n  gather(-carat, key = \"var\", value = \"value\") %>% \n  ggplot(aes(carat, value))+geom_point()+\n  facet_grid(facets=var~., scales=\"free_y\")"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#gestion-des-tables-complexes-avec-gather-3",
    "href": "talks/Cours visualisation/index.html#gestion-des-tables-complexes-avec-gather-3",
    "title": "Cours de visualisation des donn√©es",
    "section": "Gestion des tables complexes avec gather (3)",
    "text": "Gestion des tables complexes avec gather (3)\n\nExemple : Repr√©sentation de price et depth en fonction de carat, avec une couleur par modalit√© de cut\n\n\ndiamonds %>% select(carat,price,depth, cut) %>% \n  gather(-carat, -cut, key = \"var\", value = \"value\") %>% \n  ggplot(aes(carat, value, color=cut))+geom_point()+\n  facet_grid(facets=var~., scales=\"free_y\")"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#tp-exercice-4-avec-avec-ggplot2-scales-dplyr-et-tidyr",
    "href": "talks/Cours visualisation/index.html#tp-exercice-4-avec-avec-ggplot2-scales-dplyr-et-tidyr",
    "title": "Cours de visualisation des donn√©es",
    "section": "TP : Exercice 4 avec avec ggplot2, scales, dplyr et tidyr",
    "text": "TP : Exercice 4 avec avec ggplot2, scales, dplyr et tidyr\n\nReprendre le graphique de la distribution des esp√®ces en barre empil√©e cr√©√© dans l‚Äôexercice 1.\n\nChanger la position de la l√©gende\nChanger le titre de la l√©gende\n\nRepr√©senter les nuages de points des trois variables Sepal.Length, Petal.Width et Petal.Length en fonction de la variable Sepal.Width.\nReprendre le graphique pr√©c√©dent et colorer les points suivant les esp√®ces."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#r√©ordonner-les-modalit√©s",
    "href": "talks/Cours visualisation/index.html#r√©ordonner-les-modalit√©s",
    "title": "Cours de visualisation des donn√©es",
    "section": "R√©ordonner les modalit√©s",
    "text": "R√©ordonner les modalit√©s\n\nLibrairie forcats : permet de r√©ordonner les modalit√©s d‚Äôune variable selon les valeurs d‚Äôune autre variable.\nDeux fonctions :\n\nfct_reorder() : r√©partition par ordre croissant ou d√©croissant,\nfct_shuffle() : r√©partition al√©atoire.\n\nExemple : repr√©sentation de la moyenne de price en fonction des valeurs de cut, en r√©ordonnant les valeurs de la table r√©sum√©e df suivant les modalit√©s de cut\n\n\ndf = diamonds %>%\n    group_by(cut) %>%\n    summarise(\n        mean = mean(price, na.rm = T),\n        sd = sd(price, na.rm = T)\n    )"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#r√©ordonner-les-modalit√©s-2",
    "href": "talks/Cours visualisation/index.html#r√©ordonner-les-modalit√©s-2",
    "title": "Cours de visualisation des donn√©es",
    "section": "R√©ordonner les modalit√©s (2)",
    "text": "R√©ordonner les modalit√©s (2)\n\nOrdre original\n\n\nggplot(df, aes(cut, mean))+geom_point()"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#r√©ordonner-les-modalit√©s-3",
    "href": "talks/Cours visualisation/index.html#r√©ordonner-les-modalit√©s-3",
    "title": "Cours de visualisation des donn√©es",
    "section": "R√©ordonner les modalit√©s (3)",
    "text": "R√©ordonner les modalit√©s (3)\n\nR√©ordonner les modalit√©s de cut par ordre croissant de la moyenne de price\n\n\nlibrary(forcats)\nggplot(df, aes(fct_reorder(cut, mean), mean))+geom_point()"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#r√©ordonner-les-modalit√©s-4",
    "href": "talks/Cours visualisation/index.html#r√©ordonner-les-modalit√©s-4",
    "title": "Cours de visualisation des donn√©es",
    "section": "R√©ordonner les modalit√©s (4)",
    "text": "R√©ordonner les modalit√©s (4)\n\nR√©ordonner par ordre d√©croissant de la moyenne\n\n\nggplot(\n  df, \n  aes(fct_reorder(cut, mean, .desc = TRUE), mean)\n  )+\n  geom_point()"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#r√©ordonner-les-modalit√©s-5",
    "href": "talks/Cours visualisation/index.html#r√©ordonner-les-modalit√©s-5",
    "title": "Cours de visualisation des donn√©es",
    "section": "R√©ordonner les modalit√©s (5)",
    "text": "R√©ordonner les modalit√©s (5)\n\nR√©ordonner al√©atoirement les modalit√©s de cut\n\n\nggplot(df, aes(fct_shuffle(cut), mean))+geom_point()"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#gestion-de-la-repr√©sentation-simultan√©e",
    "href": "talks/Cours visualisation/index.html#gestion-de-la-repr√©sentation-simultan√©e",
    "title": "Cours de visualisation des donn√©es",
    "section": "Gestion de la repr√©sentation simultan√©e",
    "text": "Gestion de la repr√©sentation simultan√©e\nTrois syntaxes donnant le m√™me r√©sultat :\n\ndonn√©es et variable sp√©cifi√©es pour l‚Äôensemble des couches.\n\n\nggplot(diamonds, aes(x = price)) + geom_histogram()\n\n\ndonn√©es sp√©cifi√©es pour l‚Äôensemble des couches, variables √† sp√©cifier dans chaque couche.\n\n\nggplot(diamonds) + \n  geom_histogram(mapping = aes(x = price))\n\n\ndonn√©es et variables √† sp√©cifier dans chaque couche.\n\n\nggplot() + \n  geom_histogram(data = diamonds, mapping = aes(x = price))\n\n\nUtile pour repr√©senter sur le m√™me graphique des donn√©es issues de tables diff√©rentes."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#gestion-de-la-repr√©sentation-simultan√©e-2",
    "href": "talks/Cours visualisation/index.html#gestion-de-la-repr√©sentation-simultan√©e-2",
    "title": "Cours de visualisation des donn√©es",
    "section": "Gestion de la repr√©sentation simultan√©e (2)",
    "text": "Gestion de la repr√©sentation simultan√©e (2)\n\nExemple : ajouter la moyenne, ainsi qu‚Äôun indicateur de dispersion (avec geom_errorbar()) sur les bo√Ætes √† moustaches du prix suivant la variable cut.\n\n\nggplot(diamonds, aes(cut, price, color = cut)) +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  geom_boxplot(show.legend = FALSE) +\n  geom_errorbar(data = df, \n                mapping = aes(y = mean, \n                              ymin = mean - sd, \n                              ymax = mean + sd), \n                col = \"gray70\", width = .4, size = 1) +\n  geom_point(data = df, \n             mapping = aes(y = mean), \n             col = \"steelblue\", size = 2) +\n  theme_light()"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#gestion-de-la-repr√©sentation-simultan√©e-3",
    "href": "talks/Cours visualisation/index.html#gestion-de-la-repr√©sentation-simultan√©e-3",
    "title": "Cours de visualisation des donn√©es",
    "section": "Gestion de la repr√©sentation simultan√©e (3)",
    "text": "Gestion de la repr√©sentation simultan√©e (3)\n\nR√©sultat du code pr√©c√©dent"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#tp-exercice-5-avec-ggplot2-scales-et-forcats",
    "href": "talks/Cours visualisation/index.html#tp-exercice-5-avec-ggplot2-scales-et-forcats",
    "title": "Cours de visualisation des donn√©es",
    "section": "TP : Exercice 5 avec ggplot2, scales et forcats",
    "text": "TP : Exercice 5 avec ggplot2, scales et forcats\n\nReprendre les 3 histogrammes par esp√®ce cr√©√©s dans l‚Äôexercice 3. Ajouter une ligne verticale repr√©sentant la moyenne pour chaque esp√®ce (indication : geom_vline).\nCr√©er une table de donn√©es df r√©sumant la variable Sepal.Length par ses moyennes et √©cart-types par esp√®ce.\nReprendre les bo√Ætes √† moustaches cr√©es dans l‚Äôexercice 1. Colorer chaque bo√Æte √† moustaches et enlever la l√©gende.\nAjouter sur chaque bo√Æte √† moustache la moyenne (sous forme de point) et l‚Äô√©cart-type (sous forme de barre).\nRefaire le graphique pr√©c√©dent en r√©ordonnant les modalit√©s par moyenne d√©croissante."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#tp-exercice-6-avec-ggplot2-dplyr-tidyr-et-readr",
    "href": "talks/Cours visualisation/index.html#tp-exercice-6-avec-ggplot2-dplyr-tidyr-et-readr",
    "title": "Cours de visualisation des donn√©es",
    "section": "TP : Exercice 6 avec ggplot2, dplyr, tidyr et readr",
    "text": "TP : Exercice 6 avec ggplot2, dplyr, tidyr et readr\n\nReprendre les donn√©es de la banque mondiale. Cr√©er deux tables (une large et une longue) avec uniquement les valeurs estim√©es. Cr√©er un indicateur de donn√©es manquantes dans la table longue.\nRepr√©senter sur un graphique l‚Äô√©volution du pourcentage de donn√©es manquantes en fonction de l‚Äôann√©e. Indication : changer les √©tiquettes de la variable annee en valeurs num√©riques\n\n\nas.numeric(substring(annee, 2))\n\n\nCr√©er la table longue restreinte aux valeurs du contr√¥le de la corruption et repr√©senter sa distribution en fonction de l‚Äôann√©e.\nRepr√©senter la courbe de l‚Äô√©volution en moyenne en fonction de l‚Äôann√©e. Mettre des noms et un titre appropri√©s au graphique.\nAjouter au graphique pr√©c√©dent l‚Äô√©volution des pays ‚ÄúFrance‚Äù, ‚ÄúSomalia‚Äù et ‚ÄúDenmark‚Äù, avec une l√©gende correcte ainsi qu‚Äôune couleur par pays."
  },
  {
    "objectID": "Blog projets/posts/post-with-code/index.html",
    "href": "Blog projets/posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Blog projets/posts/welcome/index.html",
    "href": "Blog projets/posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "Blog projets/index.html",
    "href": "Blog projets/index.html",
    "title": "Blog projets",
    "section": "",
    "text": "news\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2022\n\n\nTristan O‚ÄôMalley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Blog projets/about.html",
    "href": "Blog projets/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#cartographie",
    "href": "talks/Cours visualisation/index.html#cartographie",
    "title": "Cours de visualisation des donn√©es",
    "section": "Cartographie",
    "text": "Cartographie\n\nCarte choropl√®the : carte g√©ographique dont chaque zone est color√©e selon une mesure statistique.\nEtape 1 : charger les donn√©es g√©ographiques, par exemple directement sur le web avec geojson_read()\n\n\nlibrary(geojsonio)\netats = geojson_read(\n  'https://datahub.io/core/geo-countries/r/countries.geojson',\n  what = \"sp\")\n\n\no√π l‚Äôoption what r√®gle le format d‚Äôimportation. Ici, on importe au format ‚Äúspatial‚Äù.\nIci, etats contient toutes les informations n√©cessaires √† la constitution de la carte (latitudes, longitudes, noms des pays, etc‚Ä¶)"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#cartographie-2",
    "href": "talks/Cours visualisation/index.html#cartographie-2",
    "title": "Cours de visualisation des donn√©es",
    "section": "Cartographie (2)",
    "text": "Cartographie (2)\n\nEtape 2 : repr√©sentation de la carte. Exemple avec ggplot2 (apr√®s convertion de la table pour ggplot2 avec fortify)\n\n\netats_f = fortify(etats)\nggplot(etats_f, aes(long, lat, group = group)) +\n  geom_polygon(color = \"white\") +\n  theme_void()\n\no√π theme_void() indique un th√®me vide."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#cartographie-3",
    "href": "talks/Cours visualisation/index.html#cartographie-3",
    "title": "Cours de visualisation des donn√©es",
    "section": "Cartographie (3)",
    "text": "Cartographie (3)\n\nEtape 3 : colorer chaque zone en fonction d‚Äôune mesure statistique. Ici on associe √† chaque zone une valeur al√©atoire entre \\(0\\) et \\(1\\)\n\n\nlibrary(tibble) # pour rownames_to_column\netats@data$var = runif(nrow(etats@data))\netats_f = fortify(etats)\netats_fj = etats_f %>%\n  inner_join(etats@data %>% rownames_to_column(\"id\"), \n             by = \"id\")\nggplot(etats_fj, \n       aes(long, lat, group = group, fill = var)\n       ) +\n  geom_polygon(color = \"white\") +\n  theme_void()"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#cartographie-avec-leaflet",
    "href": "talks/Cours visualisation/index.html#cartographie-avec-leaflet",
    "title": "Cours de visualisation des donn√©es",
    "section": "Cartographie avec leaflet",
    "text": "Cartographie avec leaflet\n\nPersonnalisation et interaction utilisateur avec la librairie leaflet (voir ici pour une pr√©sentation compl√®te de la librairie).\nCr√©ation d‚Äôune carte en 3 √©tapes :\n\nEtape 1 : cr√©ation de la carte avec leaflet(),\nEtape 2 : ajout d‚Äôun fond de carte avec addTiles() (voir ici pour les diff√©rents fonds de carte),\nEtape 3 : ajout des polygones avec addPolygons().\n\nExemple avec les donn√©es g√©ographiques etats\n\n\nlibrary(leaflet)\nleaflet(etats) %>% \n  addTiles() %>%\n  addPolygons()\n\n\nRemarque : avec leaflet, ici pas besoin de re-configurer les donn√©es."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#cartographie-avec-leaflet-2",
    "href": "talks/Cours visualisation/index.html#cartographie-avec-leaflet-2",
    "title": "Cours de visualisation des donn√©es",
    "section": "Cartographie avec leaflet (2)",
    "text": "Cartographie avec leaflet (2)\n\nAssociation des mesures statistiques √† une palette de couleur avec la fonction colorBin()\n\n\npalette = colorBin(\"YlOrRd\", domain = etats$var)\n\n\no√π\n\n‚ÄúYlOrd‚Äù d√©finit la palette de couleurs (ici entre jaune et rouge, voir color brewer ou veridis pour d‚Äôautres choix de palettes),\ndomain definit les mesures √† associer aux couleurs.\n\nRepr√©sentation de la carte choropl√®the en indiquant que la couleur de remplissage est fonction de la palette et de la variable avec fillColor, et en r√©glant l‚Äôopacit√© des couleurs avec fillOpacity\n\n\nleaflet(etats) %>% \n  addTiles() %>%\n  addPolygons(fillColor = ~palette(var), \n              fillOpacity = .5)"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#cartographie-avec-leaflet-3",
    "href": "talks/Cours visualisation/index.html#cartographie-avec-leaflet-3",
    "title": "Cours de visualisation des donn√©es",
    "section": "Cartographie avec leaflet (3)",
    "text": "Cartographie avec leaflet (3)\n\nAm√©lioration de la carte pr√©c√©dente : personnalisation de la palette\n\n\npalette = colorBin(\n  \"magma\", \n  domain = etats$var, \n  bins = seq(0, 1, by = .2)\n  )\n\n\no√π l‚Äôoption bins d√©finit le d√©coupage des valeurs de la mesure en classes, chaque classe √©tant ensuite associ√©e √† une couleur (valeur par d√©faut = \\(7\\))."
  },
  {
    "objectID": "talks/Cours visualisation/index.html#cartographie-avec-leaflet-4",
    "href": "talks/Cours visualisation/index.html#cartographie-avec-leaflet-4",
    "title": "Cours de visualisation des donn√©es",
    "section": "Cartographie avec leaflet (4)",
    "text": "Cartographie avec leaflet (4)\n\nleaflet(etats) %>% \n  setView(lat = 0, lng = 0, zoom = 1) %>%\n  addTiles() %>%\n  addPolygons(\n    fillColor = ~palette(var), \n    fillOpacity = .5,\n    color = \"gray30\", \n    weight = 1.5, \n    opacity = 1, \n    dashArray = \"2\"\n  ) %>%\n  addLegend(\n    pal = palette, \n    values = ~var, \n    opacity = 0.8, \n    title = \"Variable aleatoire\",\n    position = \"bottomright\"\n  )"
  },
  {
    "objectID": "talks/Cours visualisation/index.html#tp-exercice-7-avec-geojsonio-leaflet-tibble-dplyr-et-readr",
    "href": "talks/Cours visualisation/index.html#tp-exercice-7-avec-geojsonio-leaflet-tibble-dplyr-et-readr",
    "title": "Cours de visualisation des donn√©es",
    "section": "TP : Exercice 7 avec geojsonio, leaflet, tibble, dplyr et readr",
    "text": "TP : Exercice 7 avec geojsonio, leaflet, tibble, dplyr et readr\n\nRreprendre les donn√©es et les graphiques de l‚Äôexercice 6. Repr√©senter sur une carte la valeur du contr√¥le de la corruption en 2016 pour chaque pays. Ajouter au graphique un titre et des l√©gendes convenables."
  }
]